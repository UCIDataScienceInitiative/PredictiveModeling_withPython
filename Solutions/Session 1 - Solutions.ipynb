{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Predictive Modeling with Python:  Linear Regression\n",
    "#### Author: Brian Vegetabile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goals of this Lesson\n",
    " \n",
    "- Load in a baseball dataset. We'll be trying to predict players' salaries based on their season statistics (home runs, batting average, etc.)\n",
    "  - Explore the data: simple plotting with `matplotlib`\n",
    "- Present the fundamentals of linear regression for predictive modeling\n",
    "    - Notation and Framework\n",
    "    - Closed form matrix solutions for linear regression\n",
    "        - Advantages and issues\n",
    "    - Gradient descent for linear regression\n",
    "        - Advantages and Issues\n",
    "        - Code gradient descent to model player salaries  \n",
    "    - Perform linear regression in `scikit-learn'\n",
    "\n",
    "\n",
    "#### References for Linear Regression\n",
    "\n",
    "\n",
    "- Elements of Statistical Learning by Hastie, Tibshriani, Friedman - Chapter 3 \n",
    "- Alex Ihler's Course Notes on Linear Models for Regression - http://sli.ics.uci.edu/Classes/2015W-273a\n",
    "- scikit-learn Documentation - http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\n",
    "- Linear Regression Analysis By Seber and Lee - http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471415405,subjectCd-ST24.html\n",
    "- Applied Linear Regression by Weisberg - http://onlinelibrary.wiley.com/book/10.1002/0471704091\n",
    "- Wikipedia - http://en.wikipedia.org/wiki/Linear_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# I. Exploring the dataset\n",
    "\n",
    "We'll use this dataset to investigate Linear Regression.  The dataset consists of 337 observations and 18 variables from the set of Major League Baseball players who played at least one game in both the 1991 and 1992\n",
    "seasons, excluding pitchers.  The dataset contains the 1992 salaries for that population, along with performance measures for each player.  Four categorical variables indicate how free each player was to move to other teams.\n",
    "\n",
    "** Reference **\n",
    "\n",
    "- Pay for Play: Are Baseball Salaries Based on Performance?\n",
    "    - http://www.amstat.org/publications/jse/v6n2/datasets.watnik.html\n",
    "\n",
    "**Filename**\n",
    "\n",
    "- 'baseball.dat.txt'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** What we will try to predict **\n",
    "\n",
    "We will attempt to predict the players salary based upon some predictor variables such as Hits, OBP, Walks, RBIs, etc. \n",
    "\n",
    "**Variables**\n",
    "\n",
    "- _Salary_: Thousands of dollars\n",
    "- _AVG_: Batting average\n",
    "- _OBP_: On-base percentage\n",
    "- _Runs_: Number of runs\n",
    "- _Hits_: Number of hits\n",
    "- _Doubles_: Number of doubles\n",
    "- _Triples_: Number of triples\n",
    "- _HR_: Number of home runs\n",
    "- _RBI_: Number of runs batted in\n",
    "- _Walks_: Number of walks\n",
    "- _SO_: Number of strike-outs\n",
    "- _SB_: Number of stolen bases\n",
    "- _Errs_: Number of errors\n",
    "- _free agency eligibility_: Indicator of \"free agency eligibility\"\n",
    "- _free agent in 1991/2_: Indicator of \"free agent in 1991/2\"\n",
    "- _arbitration eligibility_: Indicator of \"arbitration eligibility\"\n",
    "- _arbitration in 1991/2_: Indicator of \"arbitration in 1991/2\"\n",
    "- _Name_: Player's name (in quotation marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "#### Load The Data\n",
    "\n",
    "Loading data in python from csv files in python can be done by a few different ways.  The numpy package has a function called 'genfromtxt' that can read csv files, while the pandas library has the 'read_csv' function.  Remember that we have imported numpy and pandas as `np` and `pd` respectively at the top of this notebook.  An example using pandas is as follows:\n",
    "\n",
    "    pd.read_csv(filename, **args)\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.parsers.read_csv.html\n",
    "\n",
    "\n",
    "### <span style=\"color:purple\">STUDENT ACTIVITY (2 MINS) </span> \n",
    "\n",
    "<span style=\"color: purple\"> 1. Load the 'baseball.dat.txt' file into a variable called 'baseball'. </span>\n",
    "\n",
    "<span style=\"color: purple\"> 2. Use baseball.head() to view the first few entries.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "# Fill in the code below- Load the file 'baseball.dat.txt' using pd.read_csv() #\n",
    "################################################################################\n",
    "\n",
    "baseball = pd.read_csv('data/baseball.dat.txt')\n",
    "baseball.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Crash Course: Plotting with Matplotlib**\n",
    "\n",
    "At the top of this notebook we have imported the the package `pyplot as plt` from the `matplotlib` library.  `matplotlib` is a great package for creating simple plots in Python.  Below is a link to their tutorial for basic plotting.\n",
    "\n",
    "_Tutorials_\n",
    "\n",
    "- http://matplotlib.org/users/pyplot_tutorial.html\n",
    "- https://scipy-lectures.github.io/intro/matplotlib/matplotlib.html\n",
    "\n",
    "_Simple Plotting_\n",
    "\n",
    "_NOTE: This may not always be the best way to create plots, but it is a quick template to get you started._\n",
    "\n",
    "\n",
    "1. Import the packge pyplot from matplotlib for plotting \n",
    "\n",
    "    ```python\n",
    "    import matplotlib.pyplot as plt\n",
    "    ```\n",
    "2. Create a variable to store a new figure object\n",
    "\n",
    "    ```python\n",
    "    fig = plt.figure()\n",
    "    ```\n",
    "    \n",
    "3. Create the plot of your choice\n",
    "    \n",
    "    `plt.plot(x,y)` - A line plot\n",
    "    \n",
    "    `plt.scatter(x,y)` - Scatter Plots\n",
    "    \n",
    "    `plt.hist(x)` - Histogram of a variable\n",
    "    \n",
    "     Example Plots: http://matplotlib.org/gallery.html\n",
    "     \n",
    "4. Create labels for your plot for better interpretability\n",
    "\n",
    "    X axis label: `plt.xlabel('String')`\n",
    "    \n",
    "    Y axis Label: `plt.ylabel('String')`\n",
    "    \n",
    "    Title: `plt.title('String')`\n",
    "\n",
    "5. Change the figure size for better viewing within the iPython Notebook\n",
    "    ```python\n",
    "    fig.set_size_inches(width, height)\n",
    "    ```\n",
    "    \n",
    "6. Display the plot in the notebook\n",
    "    ```python\n",
    "    plt.show()\n",
    "    ```\n",
    "    The above command allows the plot to be shown below the cell that you are currently in.  This is made possible by using the command `%matplotlib inline` back when we imported `matplotlib.pyplot`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Demonstration - Plot a Histogram of Hits #\n",
    "############################################\n",
    "\n",
    "f = plt.figure()\n",
    "plt.hist(baseball['Hits'], bins=15)\n",
    "plt.xlabel('Number of Hits')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Number of Hits')\n",
    "f.set_size_inches(10, 5)   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:purple\">STUDENT ACTIVITY (7 MINS)</span> \n",
    "\n",
    "\n",
    "Work in pairs to import the package `matplotlib.pyplot` and create the following two plots. \n",
    "\n",
    "- A histogram of the $log(Salary)$\n",
    "    \n",
    "    Hint: To perform the logarithmic transformation on all elements in a numpy array, use the command\n",
    "    ```python\n",
    "    np.log(array)\n",
    "    ```\n",
    "- A scatterplot of $log(Salary)$ on the x-axis vs $Hits$ on the y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Student Action - Plot a Histogram of log(Salaries) #\n",
    "######################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = plt.figure()\n",
    "plt.hist(np.log(baseball['Salary']), bins=15)\n",
    "plt.xlabel('log(Player Salary)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Player Salary (Thousands of Dollars)')\n",
    "f.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Student Action - Plot a Scatter Plot of Salaries vs. Hitting #\n",
    "################################################################\n",
    "\n",
    "f = plt.figure()\n",
    "plt.scatter(baseball['Hits'], np.log(baseball['Salary']))\n",
    "plt.xlabel('Number of Hits')\n",
    "plt.ylabel('log(Player Salary)')\n",
    "f.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# II. Predictive modeling using linear regression\n",
    "\n",
    "## Notation and framework\n",
    "\n",
    "**Linear regression**: a technique for predictive modeling that uses a linear combination of _features_ to predict a continuous outcome variable. We model the outcomes using the functional form below:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\\begin{equation*}\n",
    "\\hat y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_D x_D = \\mathbf{x}  \\boldsymbol \\beta\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\hat y &:& \\mbox{response or target variable} \\\\\n",
    "\\mathbf{x} &:& \\mbox{row vector of predictor feature variables } \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{x}  = (1, x_1, \\dots, x_D) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\boldsymbol\\beta = (\\beta_0, \\beta_1, \\dots, \\beta_D) : \\mbox{Model parameters, column vector} \n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "For this baseball dataset exampe, we are interested in predicting player salaries given their batting average, number of hits, on base percentage, and other statistics. In this case, \n",
    "\n",
    "* The response variable $\\mathbf{\\hat y}$ is the log of the salary\n",
    "* Each of the other variables (HRs, batting average, ...) would correspond to an element $x_1, \\dots, x_D$ in the feature vector $\\mathbf{x}$. \n",
    "* We want to find the set of model parameters $\\boldsymbol\\beta$ that take the feature vector $\\mathbf{x}$ and give the best predictions for player salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "As a simplified example, let's say that we are only interested in predicting salary given a player's number of hits:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\rm{\\log salary} = \\beta_0 + \\beta_1 \\rm{(\\#~hits)} \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "To write this more succinctly we can use the variables that we discussed which correspond to our feature vector $\\mathbf{x}=(x_1 , \\dots, x_D)$, prediction $\\hat y$, and model coefficients $\\boldsymbol\\beta=(\\beta_0, \\beta_1, \\dots)$:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\hat y = \\beta_0 + \\beta_1 x_1 \n",
    "\\end{eqnarray*}\n",
    "where\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log \\rm{salary} \\rightarrow \\hat y, \\rm{~ ~ ~\\# ~ hits} \\rightarrow x_1\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "If we were only using one variable (number of hits) to predict a player's salary, then the parameters $\\beta_0$ and $\\beta_1)$ of our model would be the y-intercept and the slope of a straight line fit to the data in the scatter plot we just made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** A more general approach: linear regression with multiple features **\n",
    "\n",
    "We can greatly improve the accuracy of our model by adding in additional variables that correlate with player quality. In this case, our prediction for $y$ has more terms:\n",
    "\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\hat y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\dots\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where the additional features $x_2, x_3, \\dots$ would be other statistics from the baseball season, such as batting average, number of home runs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "** Why is it called linear regression? **\n",
    "\n",
    "Our model _does not_ need to be linear in the raw data variables, i.e. we could construct a model with terms like $x_1=\\rm{(\\#~hits)^2}$, $x_2=\\rm{\\log(home~runs)}$, and so on where features $x_1,\\dots,x_D$ are now polynomials or transformations of the original data provided. That is, a model such as \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\hat y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1^3 + \\beta_4 \\sin(x_1)\n",
    "\\end{equation*}\n",
    "\n",
    "is still a linear regression. The term _linear_ applies to the learned coefficients $\\beta$ and not the input features $\\mathbf{x}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## How do we learn the model parameters $\\beta$? \n",
    "\n",
    "We want our best-fitting model to provide predictions that are as close as possible to the actual values of the response variable $y$, i.e. we try to minimze the error between the prediction $\\hat y$ and the observed data $y$\n",
    "\n",
    "Note the notation used here where <u>the subscript $i$ refers to data point $i$</u>. In our example, each player is one data point, so different $i's$ index different players).\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "    \\hat y_i &=& \\rm{value ~predicted ~by ~the ~model}    \\\\\n",
    "    y_i &=&  \\rm{actual~ value ~of ~response ~variable ~from ~the ~data} \\\\\n",
    "    \\rm{error_i} &=&  y_i - \\hat y_i \n",
    "\\end{eqnarray*}\n",
    "\n",
    "The straight line in the first plot below is an example of the linear model provides the best fit to the data points. The red lines show the magnitude of the errors/residuals in this model. The second plot shows what residuals in a badly fitting model look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Demonstration - What do Residuals Look Like (Best model)  #\n",
    "#############################################################\n",
    "\n",
    "# we know this is the 'best' model because we cheated and used it to construct our data set\n",
    "\n",
    "np.random.seed(33)     # Setting a seed allows reproducability of experiments\n",
    "\n",
    "beta0 = 1              # Creating an intercept\n",
    "beta1 = 0.5            # Creating a slope\n",
    "\n",
    "# Randomly sampling data points\n",
    "x_example = np.random.uniform(0,5,10)\n",
    "y_example = beta0 + beta1 * x_example + np.random.normal(0,.2,10)\n",
    "line1 = beta0 + beta1 * np.arange(-1, 6)\n",
    "\n",
    "f = plt.figure()\n",
    "plt.scatter(x_example,y_example)   # Plotting observed data\n",
    "plt.plot(np.arange(-1,6), line1)   # Plotting the true line\n",
    "\n",
    "\n",
    "for i, xi in enumerate(x_example):\n",
    "    plt.vlines(xi, beta0 + beta1 * xi, y_example[i], colors='red') # Plotting Residual Lines\n",
    "    \n",
    "    \n",
    "plt.annotate('Error or \"residual\"', xy = (x_example[5], 1.56), xytext = (-1.5,2.1),\n",
    "             arrowprops=dict(width=1,headwidth=7,facecolor='black', shrink=0.01))\n",
    "f.set_size_inches(10,5)\n",
    "plt.title('Errors in Linear Regression (best model)', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "#############################################################\n",
    "# Demonstration - What do Residuals Look Like (bad model)\n",
    "#############################################################\n",
    "\n",
    "\n",
    "a,b=1.3, 0.3           # make intercept and slope in model worse by these factors\n",
    "\n",
    "line2 = a*beta0 + beta1*b * np.arange(-1, 6)\n",
    "\n",
    "f = plt.figure()\n",
    "# Using same data points as before\n",
    "plt.scatter(x_example,y_example)   # Plotting observed data\n",
    "plt.plot(np.arange(-1,6), line2, color='orange')   # Plotting the bad fit line\n",
    "\n",
    "\n",
    "for i, xi in enumerate(x_example):\n",
    "    plt.vlines(xi, a*beta0 + b*beta1 * xi, y_example[i], colors='red') # Plotting Residual Lines\n",
    "    \n",
    "    \n",
    "f.set_size_inches(10,5)\n",
    "plt.title('Errors in Linear Regression (worse model)', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Minimizing the loss function **\n",
    "\n",
    "A **loss function** is a way to quantify how poorly a model is doing in fitting the data. \n",
    "Linear Regression can be thought of as an optimization problem where we want to minimize some loss function of the error between the prediction $\\hat y$ and the observed data $y$.\n",
    "\n",
    "\n",
    "Historically, linear regression has been solved using the method of Least Squares where we are interested in minimizing the mean squared error loss function of the form:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "    Loss(\\boldsymbol\\beta) = MSE &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat y_i)^2 \\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where $N$ is the total number of observations. In our example, $N$ is the number of baseball players in the data set.\n",
    "\n",
    "Other loss functions can be used, but using mean squared error (also referred to sum of the squared residuals in other text) has very nice properities for closed form solutions.  We will use this loss function for both gradient descent and to create a closed form matrix solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Matrix notation **\n",
    "\n",
    "Our expression for the predicted response of the $i^{\\rm th}$ data point is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat y_i = \\beta_0 + \\beta_1 x_{1, i} + \\beta_2 x_{2,i} + \\dots + \\beta_D x_{D,i}\n",
    "\\end{equation}\n",
    "\n",
    "If we plugged this into our expression for the MSE loss function, it could take a while to write. \n",
    "We can compactify the expression for the loss function by using the following matrix notation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{\\hat Y} = \\left( \\begin{array}{ccc}\n",
    "\\hat y_1 \\\\\n",
    "\\hat y_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat y_i \\\\\n",
    "\\vdots \\\\\n",
    "\\hat y_N\n",
    "\\end{array} \\right)\n",
    "\\qquad\n",
    "\\mathbf{X} = \\left( \\begin{array}{ccc}\n",
    "1 & x_{1,1} & x_{1,2} & \\dots & x_{1,D} \\\\\n",
    "1 & x_{2,1} & x_{2,2} & \\dots & x_{2,D} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{i,1} & x_{i,2} & \\dots & x_{i,D} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{N,1} & x_{N,2} & \\dots & x_{N,D} \\\\\n",
    "\\end{array} \\right)\n",
    "\\qquad\n",
    "\\beta = \\left( \\begin{array}{ccc}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_j \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_D\n",
    "\\end{array} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Note the column of 1's in the $\\mathbf{X}$ feature matrix. This is necessary because of the $\\beta_0$ intercept parameter in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Then the expression for the model predictions can be written in a much simpler form:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{\\hat Y}= \\mathbf{X } \\boldsymbol \\beta\n",
    "\\end{equation*}\n",
    "\n",
    "The prediction for the $i^{\\rm th}$ data point is \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat y_i = \\mathbf{\\hat Y_i} = \\mathbf{X_i} \\boldsymbol \\beta\n",
    "\\end{equation*}\n",
    "\n",
    "and the MSE loss function is\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Loss(\\boldsymbol \\beta) &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\mathbf{X_i} \\boldsymbol \\beta)^2 \\\\\n",
    "&=& \\frac{1}{N} (\\mathbf{ Y} -\\mathbf{X } \\boldsymbol \\beta )^\\mathbf{T} (\\mathbf{ Y} -\\mathbf{X } \\boldsymbol \\beta )\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent: algorithm for fitting a model\n",
    "\n",
    "In linear regression we are interested in optimizing our loss function $Loss(\\boldsymbol \\beta)$ to find the optimal $\\beta$ such that \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\hat \\beta &=& \\arg \\min_{\\beta} \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\mathbf{x_i }\\beta)^2 \\\\\n",
    "&=& \\arg \\min_{\\beta} \\frac{1}{N} \\mathbf{(Y - X \\beta)^T (Y - X \\beta)} \\\\\n",
    "\\end{eqnarray*}\n",
    "Note that the $\\mathbf Y$ in the above two lines is the response variable from the data, and is different from $\\mathbf {\\hat Y = X}\\boldsymbol\\beta$, the predictions from the model.\n",
    "\n",
    "One optimization technique called 'gradient descent' is useful for finding an optimal solution to this problem.  Gradient descent is a first order optimization technique that attempts to find a local minimum of a function by updating its position by taking steps proportional to the negative gradient of the function at its current point.  The gradient at the point indicates the direction of steepest ascent and is the best guess for which direction the algorithm should go.  \n",
    "\n",
    "If we consider $\\boldsymbol \\beta$ to be some parameters we are interested in optimizing, $L(\\boldsymbol \\beta)$ to be our loss function, and $\\alpha$ to be our step size proportionality, then we have the following algorithm:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Algorithm - Gradient Descent**\n",
    "\n",
    "\n",
    "* Initialize $\\boldsymbol \\beta$\n",
    "\n",
    "* While $\\alpha | \\nabla L(\\boldsymbol \\beta) | < \\rm{tolerance} $:\n",
    "\n",
    "    * $\\boldsymbol \\beta := \\boldsymbol \\beta - \\alpha \\nabla_{\\boldsymbol \\beta} L(\\boldsymbol \\beta )$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "For our problem at hand, we therefore need to find $\\nabla L(\\boldsymbol \\beta)$. The partial deriviative of $L(\\boldsymbol \\beta)$ with respect to the $j^{th}$ feature is:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "    \\frac{\\partial L(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta_j} = -\\frac{2}{N}\\sum_{i=1}^{N} (y_i - \\mathbf{X_i}\\boldsymbol \\beta)\\cdot{\\mathbf{X_{i,j}}}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "In matrix notation this can be written:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Loss(\\beta) &=& \\frac{1}{N}\\mathbf{(Y - X  \\boldsymbol\\beta)^T ( Y - X  \\boldsymbol\\beta)} \\\\\n",
    "&=& \\frac{1}{N}\\mathbf{( Y^T  Y} - 2 \\mathbf{\\boldsymbol\\beta^T X^T  Y + \\boldsymbol\\beta^T X^T  X\\boldsymbol\\beta)} \\\\\n",
    "\\nabla_{\\boldsymbol\\beta} L(\\boldsymbol\\beta) &=& \\frac{1}{N} (-2 \\mathbf{X^T  Y} + 2 \\mathbf{X^T X \\boldsymbol\\beta)} \\\\\n",
    "&=& -\\frac{2}{N} \\mathbf{X^T (Y - X \\boldsymbol\\beta)} \\\\\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:purple\">STUDENT ACTIVITY (7 MINS)</span> \n",
    "\n",
    "Create a function that returns the gradient of $L(\\boldsymbol \\beta)$.\n",
    "\n",
    "Hints:\n",
    "\n",
    "`np.dot(A,B)` returns the dot product of two arrays `A` and `B`.\n",
    "\n",
    "`A.transpose()` returns the transpose of array `A`.\n",
    "\n",
    "`len(L)` returns the length of the array `L`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Student Action - Programming the Gradient\n",
    "###################################################################\n",
    "\n",
    "def gradient(X, y, betas):\n",
    "    #****************************\n",
    "    # Your code here!\n",
    "    err = y - np.dot(X, betas)\n",
    "    return -2*np.dot(X.transpose(),err)/len(y)\n",
    "    #****************************\n",
    "    \n",
    "\n",
    "####################################################################\n",
    "# Testing your gradient function-- no need to write anything below #\n",
    "####################################################################\n",
    "np.random.seed(33)\n",
    "X = pd.DataFrame({'ones':1, \n",
    "                  'X1':np.random.uniform(0,1,50)})\n",
    "y = np.random.normal(0,1,50)\n",
    "betas = np.array([-1,4])\n",
    "grad_expected = np.array([ 2.98018138,  7.09758971])\n",
    "grad = gradient(X,y,betas)\n",
    "try:\n",
    "    np.testing.assert_almost_equal(grad, grad_expected)\n",
    "    print \"Test Passed!\"\n",
    "except AssertionError:\n",
    "    print \"*******************************************\"\n",
    "    print \"ERROR: Something isn't right... Try Again!\"\n",
    "    print \"*******************************************\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:purple\">STUDENT ACTIVITY (15 MINS)</span> \n",
    "\n",
    "Use the gradient function that you wrote above to complete the gradient descent for the baseball dataset.\n",
    "\n",
    "\n",
    "We have set-up the all necessary matrices and starting values.  In the designated section below code the algorithm from the previous section above. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up our matrices \n",
    "Y = np.log(baseball['Salary'])\n",
    "N = len(Y)\n",
    "X = pd.DataFrame({'ones' : np.ones(N), \n",
    "                  'Hits' : baseball['Hits']})\n",
    "p = len(X.columns)\n",
    "\n",
    "# Initializing the beta vector \n",
    "betas = np.array([0.015,5.13])\n",
    "\n",
    "# Initializing Alpha\n",
    "alph = 0.00001\n",
    "\n",
    "# Setting a tolerance \n",
    "tol = 1e-8\n",
    "\n",
    "###################################################################\n",
    "# Student Action - Programming the Gradient Descent Algorithm Below\n",
    "###################################################################\n",
    "\n",
    "err = Y - np.dot(X, betas)\n",
    "grad = gradient(X,Y,betas)\n",
    "niter = 1.\n",
    "while (alph*np.linalg.norm(gradient(X,Y,betas)) > tol) and (niter < 20000):\n",
    "    #****************************\n",
    "    # Your code here!\n",
    "    betas = betas - alph*gradient(X,Y,betas)\n",
    "    niter += 1\n",
    "    #****************************\n",
    "\n",
    "print niter, betas\n",
    "\n",
    "try:\n",
    "    beta_expected = np.array([ 0.01513772, 5.13000121])\n",
    "    np.testing.assert_almost_equal(betas, beta_expected)\n",
    "    print \"Test Passed!\"\n",
    "except AssertionError:\n",
    "    print \"*******************************************\"\n",
    "    print \"ERROR: Something isn't right... Try Again!\"\n",
    "    print \"*******************************************\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Comments on Gradient Descent **\n",
    "\n",
    "- Advantages\n",
    "    - Very general algorithm $\\rightarrow$ gradient descent and its variants are used throughout machine learning and statistics\n",
    "    \n",
    "- Disadvantages \n",
    "    - Highly sensitive to initial starting conditions\n",
    "    - Not guaranteed to find the global optima\n",
    "    - How do you choose step size $\\alpha$?\n",
    "        - Too small $\\rightarrow$ May never find the minima\n",
    "        - Too large $\\rightarrow$ May step past the minima\n",
    "        - Can we fix it?\n",
    "            - Adaptive step sizes\n",
    "            - Newton's Method for Optimization\n",
    "                - http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\n",
    "            - Each correction obviously comes with it's own computational considerations.\n",
    "\n",
    "See the supplementary material for any help necessary with scripting this in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing Gradient Descent to Understand its Limitations \n",
    "\n",
    "Let's try to find the value of $X$ that maximizes the following function:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = w \\times \\frac{1}{\\sqrt{2\\pi \\sigma_1^2}}  \\exp \\left( - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2}\\right) +  (1-w) \\times \\frac{1}{\\sqrt{2\\pi \\sigma_2^2}}  \\exp \\left( - \\frac{(x-\\mu_2)^2}{2\\sigma_2^2}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $w=0.3$, $\\mu_1 = 3, \\sigma_1^2=1$ and $\\mu_2 = -1, \\sigma_2^2=0.5$\n",
    "\n",
    "Let's visualize this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x1 = np.arange(-10, 15, 0.05)\n",
    "mu1 = 6.5 \n",
    "var1 = 3\n",
    "mu2 = -1\n",
    "var2 = 10\n",
    "weight = 0.3\n",
    "def mixed_normal_distribution(x, mu1, var1, mu2, var2):\n",
    "    pdf1 = np.exp( - (x - mu1)**2 / (2*var1) ) / np.sqrt(2 * np.pi * var1)\n",
    "    pdf2 = np.exp( - (x - mu2)**2 / (2*var2) ) / np.sqrt(2 * np.pi * var2)\n",
    "    return weight * pdf1 + (1-weight )*pdf2\n",
    "\n",
    "pdf = mixed_normal_distribution(x1, mu1, var1, mu2, var2)\n",
    "fig = plt.figure()\n",
    "plt.plot(x1, pdf)\n",
    "fig.set_size_inches([10,5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now let's show visualize happens for different starting conditions and different step sizes\n",
    "\n",
    "Play around with the starting position `x` and the step size `alph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def mixed_gradient(x, mu1, var1, mu2, var2):\n",
    "    grad_pdf1 =  np.exp( - (x - mu1)**2 / (2*var1) ) * ((x-mu1)/var1) / np.sqrt(2 * np.pi * var1)\n",
    "    grad_pdf2 =  np.exp( - (x - mu2)**2 / (2*var2) ) * ((x-mu2)/var2)  / np.sqrt(2 * np.pi * var2)\n",
    "    return weight * grad_pdf1 + (1-weight)*grad_pdf2\n",
    "\n",
    "# Initialize X\n",
    "x = -9\n",
    "# Initializing Alpha\n",
    "alph = 70\n",
    "# Setting a tolerance \n",
    "tol = 1e-8\n",
    "niter = 1.\n",
    "results = []\n",
    "\n",
    "while (alph*np.linalg.norm(mixed_gradient(x, mu1, var1, mu2, var2)) > tol) and (niter < 500000):\n",
    "    #****************************\n",
    "    results.append(x)\n",
    "    x = x - alph * mixed_gradient(x, mu1, var1, mu2, var2)\n",
    "    niter += 1\n",
    "    \n",
    "    #****************************\n",
    "print x, niter\n",
    "\n",
    "if niter < 500000:\n",
    "    exes = mixed_normal_distribution(np.array(results), mu1, var1, mu2, var2)\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x1, pdf)\n",
    "    plt.plot(results, exes, color='red', marker='x')\n",
    "    plt.ylim([0,0.1])\n",
    "    fig.set_size_inches([20,10])\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print('Not converged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression Matrix Solution\n",
    "\n",
    "You may have recognized that we could actually solve for $\\boldsymbol\\beta$ directly.  \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Loss(\\boldsymbol\\beta) &=& \\frac{1}{N}\\mathbf{(  Y - X\\boldsymbol\\beta)^T ( Y - X\\boldsymbol\\beta)} \\\\\n",
    "\\nabla_{\\boldsymbol\\beta} L(\\boldsymbol\\beta) &=& \\frac{1}{N} (-2 \\mathbf{X^T  Y} + 2 \\mathbf{X^T X \\beta}) \\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Setting to zero,\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "-2 \\mathbf{X^T  Y} + 2 \\mathbf{X^T X} \\boldsymbol\\beta &=& 0 \\\\\n",
    "\\mathbf{X^T X \\boldsymbol\\beta}  &=& \\mathbf{X^T  Y} \\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Note that the $\\mathbf Y$ in the above two lines is the response variable from the data. If we assume that the columns $X$ are linearly independent then\n",
    "\n",
    "\\begin{eqnarray*}\n",
    " \\boldsymbol \\beta_{\\rm optimum}  &=& \\mathbf{(X^T X)^{-1}X^T   Y} \\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This is called the _Ordinary Least Squares_ (OLS) Estimator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "** Comments on solving the loss function directly **\n",
    "\n",
    "- Advantages: \n",
    "\n",
    "    - Simple solution to code \n",
    "- Disadvantages: \n",
    "\n",
    "    - The design matrix must be full rank to invert\n",
    "        - Can be corrected with a generalized inverse solution\n",
    "    - Inverting a matrix can be a computational expensive operation\n",
    "        - If we have a design matrix that has $N$ observations and $D$ predictors, then X is $(N\\times D)$ it follows then that\n",
    "    \n",
    "    \\begin{eqnarray*}\n",
    "        \\mathbf{X^TX} \\mbox{ is of size } (D \\times N) \\times (N \\times D) = (D \\times D) \\\\\n",
    "    \\end{eqnarray*}\n",
    "    \n",
    "        - If a matrix is of size $(D\\times D)$, the computational cost of inverting it is $O(D^3)$.  \n",
    "        - Thus inverting a matrix is directly related to the number of predictors that are included in the analysis.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## <span style=\"color:purple\">STUDENT ACTIVITY (10 MINS)</span> \n",
    "Solve for $\\hat \\beta$ directly using OLS on the Baseball Dataset - 10 min\n",
    "    \n",
    "- Review the Supplementary Materials for help with Linear Algebra\n",
    "\n",
    "Hints:\n",
    "\n",
    "`np.dot(A,B)` returns the dot product of two arrays `A` and `B`.\n",
    "\n",
    "`A.transpose()` returns the transpose of array `A`.\n",
    "\n",
    "`len(L)` returns the length of the array `L`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Student Action - Programming the Gradient\n",
    "###################################################################\n",
    "\n",
    "def gradient(X, y, betas):\n",
    "    #****************************\n",
    "    # Your code here!\n",
    "    err = y - np.dot(X, betas)\n",
    "    return -2*np.dot(X.transpose(),err)/len(y)\n",
    "    #****************************\n",
    "    \n",
    "\n",
    "#########################################################\n",
    "# Testing your gradient function\n",
    "#########################################################\n",
    "np.random.seed(33)\n",
    "X = pd.DataFrame({'ones':1, \n",
    "                  'X1':np.random.uniform(0,1,50)})\n",
    "y = np.random.normal(0,1,50)\n",
    "betas = np.array([-1,4])\n",
    "grad_expected = np.array([ 2.98018138,  7.09758971])\n",
    "grad = gradient(X,y,betas)\n",
    "try:\n",
    "    np.testing.assert_almost_equal(grad, grad_expected)\n",
    "    print \"Test Passed!\"\n",
    "except AssertionError:\n",
    "    print \"*******************************************\"\n",
    "    print \"ERROR: Something isn't right... Try Again!\"\n",
    "    print \"*******************************************\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# III. Linear Regressin using `scikit-learn`\n",
    "\n",
    "As we've shown in the previous two exercises, when coding these algorithms ourselves, we must consider many things such as selecting step sizes, considering the computational cost of inverting matrices.  For many applications though, packages have been created that have taken into consideration many of these parameter selections.  We now turn our attention to the Python package for machinelearning called 'scikit-learn'.  \n",
    "\n",
    "- http://scikit-learn.org/stable/\n",
    "\n",
    "Included is the documentation for the scikit-learn implementation of Ordinary Least Squares from their linear models package\n",
    "\n",
    "- _Generalized Linear Models Documentation:_ \n",
    "    - http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\n",
    "\n",
    "- _LinearRegression Class Documentation:_  \n",
    "    - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\n",
    "\n",
    "From this we that we'll need to import the module `linear_model` using the following:\n",
    "\n",
    "    from sklearn import linear_model\n",
    "    \n",
    "Let's examine an example using the `LinearRegression` class from scikit-learn.  We'll continue with the simulated data from the beginning of the exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Example using the variables from the Residual Example\n",
    "\n",
    "** Notes ** \n",
    "\n",
    "- Calling `linear_model.LinearRegression()` creates an object of class  `sklearn.linear_model.base.LinearRegression`\n",
    "    - Defaults \n",
    "        - `fit_intercept = True`: automatically adds a column vector of ones for an intercept\n",
    "        - `normalize = False`: defaults to not normalizing the input predictors\n",
    "        - `copy_X = False`: defaults to not copying X\n",
    "        - `n_jobs = 1`: The number of jobs to use for the computation. If -1 all CPUs are used. This will only provide speedup for n_targets > 1 and sufficient large problems.\n",
    "    - Example\n",
    "        - `lmr = linear_model.LinearRegression()`\n",
    "- To fit a model, the method `.fit(X,y)` can be used\n",
    "    - X is a matrix where each row corresponds to one data point's features, i.e. the same format that we discussed above under 'Matrix Notation'\n",
    "        - This can be accomplished by creating a DataFrame using `pd.DataFrame()`\n",
    "    - Example\n",
    "        - `lmr.fit(X,y)`\n",
    "- To predict out of sample values, the method `.predict(X)` can be used\n",
    "- To see the $\\beta$ estimates use `.coef_` for the coefficients for the predictors and `.intercept` for $\\beta_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Demonstration - scikit-learn with Regression Example\n",
    "#############################################################\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "lmr = linear_model.LinearRegression()\n",
    "lmr.fit(pd.DataFrame(x_example), pd.DataFrame(y_example))\n",
    "\n",
    "xTest = pd.DataFrame(np.arange(-1,6))\n",
    "yHat = lmr.predict(xTest)\n",
    "\n",
    "f = plt.figure()\n",
    "plt.scatter(x_example, y_example)\n",
    "p1, = plt.plot(np.arange(-1,6), line1)\n",
    "p2, = plt.plot(xTest, yHat)\n",
    "plt.legend([p1, p2], ['y = 1 + 0.5x', 'OLS Estimate'], loc=2)\n",
    "f.set_size_inches(10,5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print 'beta_1=',lmr.coef_\n",
    "print 'beta_0=',lmr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:purple\">STUDENT ACTIVITY (15 MINS)</span> \n",
    "\n",
    "Use the scikit-learn method to perform a linear regression fit for predicting $\\log \\rm{Salary}$ given a players number of hits in the season. Find the intercept ($\\beta_0$) and slope ($\\beta_1$) of the best-fitting line.\n",
    "\n",
    "If you are ambitious, plot the best-fitting line on top of the scatter plot of hits vs. log(salary).\n",
    "\n",
    "If you get that done, try adding more features into your linear regression fit for $\\log \\rm{Salary}$, such as batting average and home runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Student Action - Use scikit-learn to calculate the beta coefficients  #\n",
    "#                                                                       #\n",
    "# Note: You no longer need the intercept column in your X matrix for    #\n",
    "#       sci-kit Learn.  It will add that column automatically.          #\n",
    "#########################################################################\n",
    "\n",
    "lmr2 = linear_model.LinearRegression(fit_intercept=True)\n",
    "lmr2.fit(pd.DataFrame(baseball['Hits']),Y)\n",
    "xtest = np.arange(0,200)\n",
    "ytest = lmr2.intercept_ + lmr2.coef_*xtest\n",
    "\n",
    "f = plt.figure()\n",
    "plt.scatter(baseball['Hits'], np.log(baseball['Salary']))\n",
    "plt.plot(xtest, ytest, color='r', linewidth=3)\n",
    "f.set_size_inches(10,5)\n",
    "plt.xlabel('Hits')\n",
    "plt.ylabel('Log (Salary)')\n",
    "plt.show()\n",
    " \n",
    "print 'beta_1=',lmr2.coef_\n",
    "print 'beta_0=',lmr2.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IV. Linear Regression in the Real World\n",
    "\n",
    "In the real world, Linear Regression for predictive modeling doesn't end once you've fit the model. Models are often fit and used to predict user behavior, used to quantify business metrics, or sometimes used to identify cats faces for internet points.  In that pursuit, it isn't really interesting to fit a model and assess its performance on data that has already been observed.  The real interest lies in _**how it predicts future observations!**_\n",
    "\n",
    "Often times then, we may be susceptible to creating a model that is perfected for our observed data, but that does not generalize well to new data.  In order to assess how we perform to new data, we can _score_ the model on both the old and new data, and compare the models performance with the hope that the it generalizes well to the new data. After lunch we'll introduce some techniques and other methods to better our chances of performing well on new data. \n",
    "\n",
    "Before we break for lunch though, let's take a look at a simulated dataset to see what we mean...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Situation**\n",
    "\n",
    "Imagine that last year a talent management company managed 400 celebrities and tracked how popular they were within the public eye, as well various predictors for that metric.  The company is now interested in managing a few new celebrities, but wants to sign those stars that are above a certain 'popularity' threshold to maintain their image.\n",
    "\n",
    "Our job is to predict how popular each new celebrity will be over the course of the coming year so that we make that best decision about who to manage. For this analysis we'll use a function `l2_error` to compare our errors on a training set, and on a test set of celebrity data.\n",
    "\n",
    "The variable `celeb_data_old` represents things we know about the previous batch of celebrities.  Each row represents one celeb.  Each column represents some tangible measure about them -- their age at the time, number of Twitter followers, voice squeakiness, etc.  The specifics of what each column represents aren't important.\n",
    "\n",
    "Similarly, `popularity_score_old` is a previous measure of the celebrities popularity.\n",
    "\n",
    "Finally, `celeb_data_new` represents the same information that we had from `celeb_data_old` but for the new batch of internet wonders that we're considering.\n",
    "\n",
    "How can we predict how popular the NEW batch of celebrities will be ahead of time so that we can decide who to sign? And are these estimates stable from year to year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with np.load('data/mystery_data_old.npz') as data:\n",
    "    celeb_data_old = data['celeb_data_old']\n",
    "    popularity_old = data['popularity_old']\n",
    "    celeb_data_new = data['celeb_data_new']\n",
    "\n",
    "lmr3 = linear_model.LinearRegression()\n",
    "lmr3.fit(celeb_data_old, popularity_old)\n",
    "predicted_popularity_old = lmr3.predict(celeb_data_old)\n",
    "predicted_popularity_new = lmr3.predict(celeb_data_new)\n",
    "\n",
    "def l2_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate the sum of squared errors (a.k.a. \"L2 error\") \n",
    "    given a vector of true ys and a vector of predicted ys\n",
    "    \"\"\"\n",
    "    diff = (y_true-y_pred)\n",
    "    return np.sqrt(np.dot(diff, diff))\n",
    "\n",
    "print \"Predicted L2 Error:\", l2_error(popularity_old, predicted_popularity_old)\n",
    "\n",
    "# print \"Popularity prediction for the first 10 new cat wranglers:\"\n",
    "# print predicted_profits[:10, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Checking How We Did\n",
    "At the end of the year, we tally up the popularity numbers for each celeb and check how well we did on our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with np.load('data/mystery_data_new.npz') as data:\n",
    "    popularity_new = data['popularity_new']\n",
    "\n",
    "print \"Predicted L2 Error:\", l2_error(popularity_new, predicted_popularity_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Something's not right... our model seems to be performing worse on this data!  Our model performed so well on last year's data, why didn't it work on the data from this year?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
