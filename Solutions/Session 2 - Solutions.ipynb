{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "53484eaa-bc29-4e80-a87f-1e506cdf0dae"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "Image(url='http://datascience.uci.edu/wp-content/uploads/sites/2/2014/09/data_science_logo_with_image1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c1acab14-edf4-4965-bf8a-0881d88aa231"
    }
   },
   "source": [
    "## Predictive Modeling with Python - Overfitting, Regularization, Model Selection\n",
    "#### Author: Kevin Bache\n",
    "#### Modified by: Preston Hinkle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3b02c9e5-ee87-40ea-9005-2bd6cf483222"
    }
   },
   "source": [
    "## Before Lunch:\n",
    "We created a linear model and saw that it performed well on already seen data but poorly on unseen data.\n",
    "\n",
    "\n",
    "This session is about creating models that *generalize* to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e0692f93-adad-4df4-ad91-5dbb7627c8d9"
    }
   },
   "source": [
    "## Cross validation\n",
    "\n",
    "\n",
    "What we saw in the previous session was a common setup.  We have $\\mathbf{X}$ and $\\mathbf{y}$ data from the past and $\\mathbf{X}$ data for the present for which we want to **predict** the future $\\mathbf{y}$ values.\n",
    "\n",
    "We can generalize this notion of past/present data into what's generally called *train* and *test* data.\n",
    "\n",
    "* **Training Data** -- A dataset that we use to train our model.  We have both $\\mathbf{X}$ and $\\mathbf{y}$\n",
    "\n",
    "* **Testing Data** -- The data set for which we are trying to predict the correct outputs $y$ on. We use the testing data to evaluate our model's performance, which informs us of how it will perform on future data.\n",
    "\n",
    "\\*\\* **Never optimize a model based on predictions of the testing set; it is there for prediction alone!.** \\*\\*\n",
    "\n",
    "If it helps, pretend that we do not know the correct $y$s in the testing set; in practice, we do not have the test $y$s.\n",
    "\n",
    "The method of training a model on one subset of the data and testing it on another is known as **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1b9918aa-3fe1-401b-ac6d-7948baead3aa"
    }
   },
   "source": [
    "## Two important concepts in machine learning:\n",
    "### <span style=\"color:green\">**1) A predictive model is only as good as its predictions on unseen data **</span>\n",
    "\n",
    "### <span style=\"color:green\">**2) Error on the dataset we trained on is not a good predictor of error on future data**</span>\n",
    "\n",
    "- Why? Because when we use data to train a model, we tend to **overfit** the model to that particular set of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c6677d6b-88aa-42eb-a495-c25e1ec8864a"
    }
   },
   "source": [
    "### Overfitting in One Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "17831af2-9219-463d-9fc1-5d8b7b9786ad"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(url='http://radimrehurek.com/data_science_python/plot_bias_variance_examples_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d943e546-e618-4f22-96cf-fbbca668aa98"
    }
   },
   "source": [
    "### Model description:\n",
    "- d = 1: Two parameters: $y = \\theta_{0} + \\theta_{1}x_{1}$ (intercept + linear term)\n",
    "- d = 2: Three parameters: $y = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2}^{2}$ (intercept + linear term + square term)\n",
    "- d = 6: Seven parameters: $y = \\theta_{0} + \\theta_{1}x_{1} + ... + \\theta_{6}x^{6}$ (intercept + linear term + ... ) a degree 6 polynomial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1b45843b-b5a7-4b4d-8782-253926f66f99"
    }
   },
   "source": [
    "### Questions:\n",
    "- Which of the above 3 models has the least training error?\n",
    "- Which do you think would have the least error on new data sampled from the same distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6f9f9047-d493-419f-841b-a9e4107f8a8a"
    }
   },
   "source": [
    "## How to Fight Overfitting?\n",
    "\n",
    "Obviously overfitting weakens our model's predictive power for new data, but how do we prevent it?\n",
    "\n",
    "There are two linked strategies to accomplish this: **regularization** and **model selection**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "66528951-162d-4f47-aac9-4836df5c17c4"
    }
   },
   "source": [
    "## Regularization\n",
    "The idea in regularization is that we're going to modify our loss function to penalize it for being too complex. Simple models tend to perform better on new data.\n",
    "\n",
    "One way to do this is to try to keep our regression coefficients small. Why would we want to do this? One intuitive explanation is that if we have big regression coefficients we'll get large changes in the predicted values from small changes in input value. This led to the erratic behavior of the 6th degree polynomial fit to the data in the above plot. Intuitively, our predictions should vary smoothly with the data.\n",
    "\n",
    "So a model with smaller coefficients makes smoother predictions.  It is simpler, which means it will have a harder time overfitting. \n",
    "\n",
    "We can change our linear regression loss function to help us reduce overfitting:\n",
    "\n",
    "### Linear Regression Loss Function\n",
    "\\begin{eqnarray*}\n",
    "    Loss(\\beta) = MSE &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat y_i)^2 \\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 \\\\   \n",
    "\\end{eqnarray*}\n",
    "\n",
    "### Linear Regression Loss Function __with Regularization__\n",
    "\\begin{eqnarray*}\n",
    "    Loss(\\beta) &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha ||\\beta||_2^2\\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\beta^T \\beta\\\\\n",
    "    &=& \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\sum_{d=1}^D \\beta_d^2\\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "**Important:** $\\alpha$ is known as the regularization *hyperparameter*, and it determines how strongly we wish to penalize large parameter values $\\|\\beta\\|^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "48413e13-33ee-482c-8b86-e3ebbd1c70c8"
    }
   },
   "source": [
    "**By the way, there are many types of regularization that can be used; we chose something called 'Ridge Regression' which uses the L$^{2}$ norm (sum of squares).**\n",
    "\n",
    "We won't get into details, but a ridge regression model can be optimized in much the same way as an unregularized linear regression: either with using some form of gradient descent or matrix-based solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d3ff3838-d669-4902-b97a-fc8f6e647ba6"
    }
   },
   "source": [
    "### <span style=\"color:red\">Understanding the effects of the regularization term</span>\n",
    "1. Partner up.  On one computer:\n",
    "  0. Construct the 'mean_squared_error' function, which calculates the average squared-distance of the predicted y-values $\\hat{y}$ and the actual y-values $y$ (see MSE above for the formula). You will be using this function throughout the rest of this session, so make sure  it is correct!\n",
    "  1. Create two linear models in scikitlearn, an unregularized `LinearRegression` model, and a regularized `Ridge` model with regularization parameter $\\alpha$ = 1. e.g., `model_ridge = Ridge(alpha = 1)`\n",
    "  2. *Train* both models on the training data set loaded below, e.g., `model.fit(x_train, y_train)`\n",
    "  3. *Predict* the output values $y$ for the training and test data set using both of the trained models, e.g., `model.predict(x)`\n",
    "  4. Calculate the mean squared error of the predictions $\\hat{y}$ for the training and test sets using the function defined below\n",
    "  5. For each model, calculate the norm (square-root of sum of squared elements) $\\|\\beta\\|=\\sqrt{\\sum_{i}\\beta_{i}^{2}}$\n",
    "      - Hints:\n",
    "          - `model.coef_` returns a list of the coefficients (list of $\\beta$s) of `model`\n",
    "          - The exponentiation operation (number\\**exponent) can act directly on numpy arrays to square each of their elements\n",
    "          - `np.sum()` returns the sum of all elements in an array. Combine this with the previous hint to easily calculate the parameter vector norm $\\|\\boldsymbol{\\beta}\\|$\n",
    "\n",
    "2. Inspect the training error, test error, and the sums of coefficients for the training and test models.\n",
    "3. Discuss; what impact did the regularization term have on our model's determined coefficients and its performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "3f839227-bb92-4e0f-a6eb-aba74fa19689"
    }
   },
   "source": [
    "##### Write the mean squared error calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "15c54c88-d6d5-4d6c-bb21-649edbcc38e3"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Insert solution below!\n",
    "######################################################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate the mean_squared_error given a vector of true ys and a vector of predicted ys\n",
    "    \"\"\"\n",
    "    diff = y_true - y_pred\n",
    "    return np.dot(diff, diff) / len(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4a79ae82-6e7a-4bf2-8b3e-62f5ba7b4bf5"
    }
   },
   "source": [
    "##### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "eaa76666-5430-483f-8921-2395b4e1ca1b"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Don't change this!\n",
    "######################################################\n",
    "\n",
    "# load overfitting data\n",
    "with np.load('data/overfitting_data.npz') as data:\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "17ae1ae1-31e2-4d80-8834-9e2edb5c0283"
    }
   },
   "source": [
    "##### Training and testing two models: an **unregularized** model and a **regularized** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5ca37ff1-22ab-444c-8406-3995e11667d2"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Insert solution below!\n",
    "######################################################\n",
    "\n",
    "###################\n",
    "# Linear regression\n",
    "###################\n",
    "\n",
    "# Train\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_train_lr = model_lr.predict(x_train)\n",
    "y_pred_test_lr = model_lr.predict(x_test)\n",
    "\n",
    "# Get errors, coefficients\n",
    "train_error_lr = mean_squared_error(y_train, y_pred_train_lr)\n",
    "test_error_lr = mean_squared_error(y_test, y_pred_test_lr)\n",
    "coefficients_norm_lr = np.sqrt(np.sum(model_lr.coef_**2.))\n",
    "\n",
    "print 'LinearRegression():'\n",
    "print '\\tMean-squared error (train)', train_error_lr\n",
    "print '\\tMean-squared error (test)', test_error_lr\n",
    "print '\\tCoefficients:', coefficients_norm_lr\n",
    "print\n",
    "\n",
    "##################\n",
    "# Ridge regression\n",
    "##################\n",
    "\n",
    "# Train\n",
    "model_ridge = Ridge(alpha = 1)\n",
    "model_ridge.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_train_ridge = model_ridge.predict(x_train)\n",
    "y_pred_test_ridge = model_ridge.predict(x_test)\n",
    "\n",
    "# Get errors, coefficients\n",
    "train_error_ridge = mean_squared_error(y_train, y_pred_train_ridge)\n",
    "test_error_ridge = mean_squared_error(y_test, y_pred_test_ridge)\n",
    "coefficients_norm_ridge = np.sqrt(np.sum(model_ridge.coef_**2.))\n",
    "\n",
    "print 'Ridge(alpha = 1):'\n",
    "print '\\tMean-squared error (train)', train_error_ridge\n",
    "print '\\tMean-squared error (test)', test_error_ridge\n",
    "print '\\tCoefficients:', coefficients_norm_ridge\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ae9a00f0-bc19-4f74-9b9c-83888e8c5399"
    }
   },
   "source": [
    "### Regularization is awesome!\n",
    "\n",
    "If the above steps were completed correctly, then you should have created and trained a *regularized* model that had less error on the test set than the *unregularized* model. Great job!\n",
    "\n",
    "Regularization is clearly a very powerful technique, but the above exercise raises some questions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8455ca52-afcb-494d-86fd-678e271b2f5d"
    }
   },
   "source": [
    "### Model selection\n",
    "\n",
    "- How do we choose $\\alpha$? The regularized model with $\\alpha=1$ was an improvement upon the unregularized model, but the value was arbitrarily chosen---surely we can do better.\n",
    "\n",
    "- One option is to try a range of $\\alpha$ values and select the one that yields the least error.\n",
    "\n",
    "- But, remember: ** we are *not* allowed to optimize our model based on the test set!** This restriction extends to hyperparameters such as $\\alpha$!\n",
    "\n",
    "- So what do we do? We need a way of validating $\\alpha$ choices, but we aren't allowed to use performance on the test data to make this decision, and the training error is no use here either...\n",
    "\n",
    "- Solution: Partition the *training dataset* into two new sets, a new *training dataset* and a *validation dataset*, and choose the parameter $\\alpha$ which yields the lowest error on the validation set!\n",
    "\n",
    "- This process is one type of **model selection** that can be used to determine the best hyperparameters to move forward with\n",
    "\n",
    "- Let's see what hyperparameter selection based on validation error looks like:\n",
    "\n",
    "\n",
    "### <span style=\"color:red\">Exercise: Choosing the optimal hyperparameter value $\\alpha$ with cross-validation</set>\n",
    "1. Partner up. On one computer:\n",
    "  1. Partition the training data set into a new training set and a validation set (this is done for you, just run the cell below)\n",
    "  2. Train 12 models with regularization parameters $\\alpha$ in {0, 1, 2, 4, 8, ..., 512, 1024} (powers of 2)\n",
    "  3. Using the `mean_squared_error` function written above, calculate the training error, validation error, test error, and the norm of the coefficient vectors $\\|\\beta\\|$ for each model. \n",
    "  4. **Store the values in lists named as follows:**\n",
    "    - 'train_errors'\n",
    "    - 'validation_errors'\n",
    "    - 'test_errors' \n",
    "    - 'coef_mags'\n",
    "    \n",
    "   - **Hints: **\n",
    "     - Some of the code is provided for you\n",
    "    - We can train a non-regularized Linear Regression model using Ridge(alpha = 0); no need to distinguish between Ridge(alpha = 0) and LinearRegression()\n",
    "    - Lists and list comprehension are *very* useful here.\n",
    "        - Create a `list` of all the alpha values, called alphas: `alphas=[0, 1, 2, ..., 1024]`\n",
    "        - Create a `list` of models like so: `models=[linear_model.Ridge(alpha) for alpha in alphas]`\n",
    "        - Iterate over every model in a `for` loop, calculating the errors and coefficients and appending them to their respective\n",
    " \n",
    "2. Inspect the relationship between $\\alpha$ and the train error, validation error, test error, and coefficient magnitudes. Describe what is happening to each as $\\alpha$ is dialed up.\n",
    "\n",
    "3. Which $\\alpha$ value works best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d6ca1524-5361-4cc5-a0f9-c375d22800a4"
    }
   },
   "source": [
    "##### Load data, create train and validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d370a59b-43fb-4cb9-ab71-af4de55848ff"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Don't change this!\n",
    "# Just evaluate this cell.\n",
    "######################################################\n",
    "\n",
    "# load overfitting data into 'train' and 'test' sets\n",
    "with np.load('data/overfitting_data.npz') as data:\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']\n",
    "\n",
    "# split the 'train' set into 'train' and 'validation' sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "validation_portion = 0.1\n",
    "seed = 1\n",
    "x_train, x_valid, y_train, y_valid = \\\n",
    "    train_test_split(x_train, y_train, test_size=validation_portion, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2d6e8f39-bb5d-45c4-b281-19c5d54aaa70"
    }
   },
   "source": [
    "##### Train, test models with varying alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "6a009835-b559-489b-9131-84d1ef53d6ce"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Insert solution below!\n",
    "######################################################\n",
    "\n",
    "alphas = [0]+[2**i for i in range(0,11)]\n",
    "\n",
    "models = [Ridge(alpha) for alpha in alphas]\n",
    "\n",
    "\n",
    "train_errors = []\n",
    "valid_errors = []\n",
    "test_errors = []\n",
    "coef_mags = []\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict the outputs y for the training, validation, and test data\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_valid_pred = model.predict(x_valid)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "    \n",
    "    # Calculate the mean-squared error of each set of predictions, and append to the appropriate lists\n",
    "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
    "    valid_errors.append(mean_squared_error(y_valid, y_valid_pred))\n",
    "    test_errors.append(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    # Calculate the norm of the coefficient vector and append to list\n",
    "    coef_mags.append(np.sum(np.dot(model.coef_, model.coef_)))\n",
    "    \n",
    "print 'training errors:' \n",
    "for i, model in enumerate(models):\n",
    "    print '\\talpha = ', alphas[i], '\\t', round(train_errors[i],3)\n",
    "    \n",
    "print '\\nvalidation errors:' \n",
    "for i, model in enumerate(models):\n",
    "    print '\\talpha = ', alphas[i], '\\t', round(valid_errors[i],3)\n",
    "print '\\t**Best alpha**', alphas[np.argmin(valid_errors)]\n",
    "    \n",
    "print '\\ntest errors:' \n",
    "for i, model in enumerate(models):\n",
    "    print '\\talpha = ', alphas[i], '\\t', round(test_errors[i],3)\n",
    "    \n",
    "print '\\ncoefficients:' \n",
    "for i, model in enumerate(models):\n",
    "    print '\\talpha = ', alphas[i], '\\t', round(coef_mags[i],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9d1fa697-4c9f-43c3-aad5-ee16011b8ffc"
    }
   },
   "source": [
    "### Now, let's plot the results!\n",
    "- Evaluate the cell below and inspect the figure that is produced.\n",
    "- *psst* did you name your lists correctly? Re-read the instructions above if the following doesn't evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "32abfdd7-d893-45b8-848e-8fbde19becdf"
    }
   },
   "outputs": [],
   "source": [
    "###### Don't change this!\n",
    "######################################################\n",
    "\n",
    "# Set up figure\n",
    "fig, axes = plt.subplots(1, 2, figsize = (16,8))\n",
    "\n",
    "# First plot\n",
    "plt.sca(axes[0])\n",
    "\n",
    "plt.semilogx(alphas[1:], train_errors[1:], lw = 3, label = 'Train', color = 'k', marker = 'o', markersize = 10)\n",
    "plt.semilogx(alphas[1:], valid_errors[1:], lw = 3, label = 'Validation', color = 'k', ls = '--', marker = 'o', markersize = 10, zorder = 3)\n",
    "plt.semilogx(alphas[1:], test_errors[1:], lw = 3, label = 'Test', color = 'k', ls = ':', marker = 'o', markersize = 10, zorder = 2)\n",
    "\n",
    "# Minimum scatter point\n",
    "plt.scatter(alphas[1+np.argsort(np.array(valid_errors[1:]))[0]], min(valid_errors), color = 'red', marker = 'o', s = 100, zorder = 4, label = r'Optimum $\\alpha$')\n",
    "\n",
    "plt.xlim(.5, 2056)\n",
    "plt.ylim(-.05,13)\n",
    "\n",
    "plt.title('Train, validation, and test errors', size = 24)\n",
    "plt.xlabel(r'Regularization parameter $\\alpha$', size = 22)\n",
    "plt.ylabel(r'Mean square error', size = 22)\n",
    "\n",
    "plt.legend(fontsize = 18, loc = 'best', numpoints=1, scatterpoints=1)\n",
    "plt.grid()\n",
    "plt.tick_params(labelsize = 18)\n",
    "\n",
    "# Second plot\n",
    "plt.sca(axes[1])\n",
    "plt.semilogx(alphas[1:], coef_mags[1:], lw = 3, color = 'k', marker = 'o', markersize = 10)\n",
    "plt.xlim(.5, 2056)\n",
    "\n",
    "\n",
    "plt.title('Total parameter magnitude', size = 24)\n",
    "plt.xlabel(r'Regularization parameter $\\alpha$', size = 22)\n",
    "plt.ylabel(r'Parameter magnitude $\\sum_{i}|\\beta_{i}|^{2}$', size = 22)\n",
    "\n",
    "plt.grid()\n",
    "plt.tick_params(labelsize = 18)\n",
    "\n",
    "\n",
    "\n",
    "# Save, show\n",
    "fig.tight_layout()\n",
    "#plt.savefig('../figures/session_2/alpha_significance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1afaaf38-d2e6-4bfb-87ba-a9c02e59e26d"
    }
   },
   "source": [
    "### Discuss the above plots\n",
    "- In the remaining time, here are some things to think about:\n",
    "    - What happens to the training error as we dial up $\\alpha$?\n",
    "    - The optimum value of $\\alpha$ is given by the location of the minimum in the validation curve. Are models to the left overfit or underfit? to the right?\n",
    "    - How is the right plot explained in terms of the Ridge loss function?\n",
    "$$Loss\\left(\\beta\\right)=\\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2 + \\alpha \\sum_{d=1}^D \\beta_d^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4aa6c4e5-ab3c-4519-b066-f1e09f278b5d"
    }
   },
   "source": [
    "## Cross validation\n",
    "\n",
    "- Using regularization with varying values of the *hyperparameter* $\\alpha$ above gave us a good way of selecting which $\\alpha$ to choose in our final model\n",
    "- However, we *still* can do better!\n",
    "- Why? We overfit our regularization hyper parameter $\\alpha$ to the validation set!\n",
    "- Remember, we calculated the validation error for each $\\alpha$ and choose the $\\alpha$ which minimized that error; if the validation data had been even *slightly* different, we might have chosen a different optimal value of $\\alpha$\n",
    "- As a demonstration, evaluate the cells below and inspect the figure that is produced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4c444ba1-e610-4272-9853-30c37c0225d4"
    }
   },
   "source": [
    "### Generating the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f3592472-8732-444a-895b-c67988979b7a"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Don't change this!\n",
    "######################################################\n",
    "\n",
    "# load overfitting data into 'train' and 'test' sets\n",
    "with np.load('data/overfitting_data.npz') as data:\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']\n",
    "\n",
    "# split the 'train' set into 'train' and 'validation' sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "validation_portion = 0.1\n",
    "seed_1 = 1234\n",
    "seed_2 = 7890\n",
    "x_train_1, x_valid_1, y_train_1, y_valid_1 = \\\n",
    "    train_test_split(x_train, y_train, test_size=validation_portion, random_state=seed_1)\n",
    "x_train_2, x_valid_2, y_train_2, y_valid_2 = \\\n",
    "    train_test_split(x_train, y_train, test_size=validation_portion, random_state=seed_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f9aafc77-cfe5-4190-b73b-2a980021c088"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Don't change this!\n",
    "######################################################\n",
    "\n",
    "alphas = sorted([0]+[2**i for i in range(0,11)])\n",
    "\n",
    "\n",
    "\n",
    "# Set up figure\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "\n",
    "lines = ['-', '--']\n",
    "\n",
    "# Loop over 2 validation sets\n",
    "for j, (x_train, y_train, x_valid, y_valid) in enumerate([(x_train_1, y_train_1, x_valid_1, y_valid_1),\n",
    "                                          (x_train_2, y_train_2, x_valid_2, y_valid_2)]):\n",
    "\n",
    "    models = [Ridge(alpha) for alpha in alphas]\n",
    "    \n",
    "    valid_errors = []\n",
    "\n",
    "    # Loop over all models\n",
    "    for model in models:\n",
    "        \n",
    "        # Fit the model to the training data\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        # Predict the outputs y for the training, validation, and test data\n",
    "        y_train_pred = model.predict(x_train)\n",
    "        y_valid_pred = model.predict(x_valid)\n",
    "        \n",
    "\n",
    "        # Calculate the mean-squared error of each set of predictions, and append to the appropriate lists\n",
    "        valid_errors.append(mean_squared_error(y_valid, y_valid_pred))\n",
    "\n",
    "        # Calculate the norm of the coefficient vector and append to list\n",
    "        coef_mags.append(np.sum(np.abs(model.coef_)))\n",
    "\n",
    "    \n",
    "\n",
    "    plt.semilogx(alphas[1:], valid_errors[1:], lw = 3, label = 'Validation set '+str(j+1), color = 'k', ls = lines[j], marker = 'o', markersize = 10)\n",
    "    plt.scatter(alphas[np.argmin(valid_errors)], np.min(valid_errors), color = 'red', s = 100, zorder = 3)\n",
    "\n",
    "# Dummy scatter call for optimum alpha in legend\n",
    "plt.scatter([],[], s = 100, color = 'red', label = r'Optimum $\\alpha$')\n",
    "    \n",
    "plt.xlim(.5, 2056)\n",
    "plt.ylim(2,10)\n",
    "\n",
    "\n",
    "# Plot cosmetics\n",
    "\n",
    "plt.title(r'$\\alpha$ vs. $MSE$ for two validation sets', size = 24)\n",
    "plt.xlabel(r'Regularization parameter $\\alpha$', size = 22)\n",
    "plt.ylabel(r'Mean square error', size = 22)\n",
    "\n",
    "plt.legend(fontsize = 18, loc = 'upper right', numpoints=1, scatterpoints=1)\n",
    "plt.grid()\n",
    "plt.tick_params(labelsize = 18)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "#plt.savefig('../figures/session_2/alpha_variance.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ac666b04-3368-4e87-bbed-69175c171cb6"
    }
   },
   "source": [
    "- The plot shows that there can be significant **variance** in the optimum $\\alpha$ as determined by cross-validation\n",
    "- How do we overcome this dilemma?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "606d7a1b-7544-4e77-8b31-b60e5d247bf4"
    }
   },
   "source": [
    "### K-Folds cross-validation\n",
    "- Here's an idea: Instead of restricting ourselves to one train/validation split, let's use multiple splits, and find which $\\alpha$ performs best *on average*\n",
    "- There are many ways of creating multiple training/validation splits, but we're going to use a method called **k-folds cross-validation**\n",
    "- Here's how it works:\n",
    "\n",
    "    1. Partition the training data into K subsets, or **folds**\n",
    "    2. For each fold k in 1 to K:\n",
    "        1. Train the model on all the data outside of fold k\n",
    "        2. Calculate the model's error on the data in fold k\n",
    "    3. Average the error for each $\\alpha$, and choose $\\hat{\\alpha}$ to be the $\\alpha$ with least error *on average*\n",
    "    4. The average error for $\\alpha=\\hat{\\alpha}$ is our best estimate for the test and future error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "6dc7f11d-dfad-47f6-9543-d85d4f63a524"
    }
   },
   "outputs": [],
   "source": [
    "Image(url='https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "88237eee-c96b-4491-bd3d-6dfaf6fb57d2"
    }
   },
   "source": [
    "* **Good:** Only wastes 100/k% of the data at a time\n",
    "* **Bad:** Takes k times long as just training one model, still wastes 100/k% of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a83dd7fa-561e-4d1d-aacd-b811487da98e"
    }
   },
   "source": [
    "##### sklearn.cross_validation.KFold example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e79149ab-d51f-4212-b80d-52a00d0bd375"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Don't change this!\n",
    "######################################################\n",
    "\n",
    "# scikit learn provides a useful object to help you perform kfold cross validation\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "n_data = len(y_train)\n",
    "fold_count = 0\n",
    "for train_reduced_row_ids, valid_row_ids in KFold(n_data, n_folds=4):\n",
    "    print\n",
    "    print\n",
    "    print(\"FOLD %d:\" % fold_count)\n",
    "    print(\"-------\")\n",
    "    print(\"train_ids:\\n%s\\n\\nvalid_ids\\n%s\" % (train_reduced_row_ids, valid_row_ids))\n",
    "    x_train_reduced = x_train[train_reduced_row_ids]\n",
    "    y_train_reduced = y_train[train_reduced_row_ids]\n",
    "    x_valid = x_train[valid_row_ids]\n",
    "    y_valid = y_train[valid_row_ids]\n",
    "    fold_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "983657d6-c1aa-4b75-9d8c-75a70f885efc"
    }
   },
   "source": [
    "#### <span style=\"color:red\">K-Fold Cross Validation Exercise</span>\n",
    "1. Partner up.  On one computer, perform K-Fold cross validation:\n",
    "    1. Create the train/validation split indices (this is done for you in the below cell, just run it!)\n",
    "    2. For each alpha in [8, 16, 32, 64, 128]:\n",
    "        1. Create a Ridge model with regularization parameter alpha\n",
    "        2. For each training/validation split\n",
    "            1. *Train* the Ridge model on the training data\n",
    "            2. *Test* the model on the validation data\n",
    "            3. Keep a running tally of the *average* validation error\n",
    "    3. Compare the *average* error for each alpha; determine the optimum value of alpha.\n",
    "    4. Is the optimum alpha from K-Folds the same as the alpha found above using a single training/test split?\n",
    "    5. After determining the optimum alpha, train a model Ridge(alpha = best_alpha) on the entire training set (combined training and validation) and calculate the test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eb573ba6-1072-431e-9d67-d1e5b81cc901"
    }
   },
   "source": [
    "##### Load the training, test data; create the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "fdd945d6-de41-41d6-a6f3-c2a5ccfbb9a4"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Don't change this!\n",
    "######################################################\n",
    "\n",
    "# load overfitting data into 'train' and 'test' sets\n",
    "with np.load('data/overfitting_data.npz') as data:\n",
    "    x_train_validation = data['x_train']\n",
    "    y_train_validation = data['y_train']\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']\n",
    "\n",
    "# Shuffle the training data in unison\n",
    "from sklearn.utils import shuffle\n",
    "x_train_validation, y_train_validation = shuffle(x_train_validation, y_train_validation, random_state = 1)\n",
    "\n",
    "# Create folds\n",
    "K = 4\n",
    "folds = KFold(x_train_validation.shape[0], n_folds = K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "80965daa-f81d-4046-920d-9c662d360821"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Insert solution below\n",
    "######################################################\n",
    "\n",
    "alphas = [2**i for i in range(3, 8)]\n",
    "\n",
    "# List of average validation error for each alpha\n",
    "avg_validation_errors = [0 for i in range(len(alphas))]\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    model = Ridge(alpha)\n",
    "    for train_rows, valid_rows in folds:\n",
    "        x_train = x_train_validation[train_rows]\n",
    "        y_train = y_train_validation[train_rows]\n",
    "        x_valid = x_train_validation[valid_rows]\n",
    "        y_valid = y_train_validation[valid_rows]\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        y_valid_predicted = model.predict(x_valid)\n",
    "        avg_validation_errors[i] += mean_squared_error(y_valid, y_valid_predicted)/K\n",
    "        \n",
    "print 'alpha\\t\\tavg. validation error'\n",
    "print '-----\\t\\t----------------'\n",
    "for i in range(len(alphas)):\n",
    "    print alphas[i], '\\t\\t', avg_validation_errors[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating the test terror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2e62cf41-ad1b-4fbb-b939-fbb2fd201641"
    }
   },
   "outputs": [],
   "source": [
    "# Reload the data\n",
    "\n",
    "# load overfitting data into 'train' and 'test' sets\n",
    "with np.load('data/overfitting_data.npz') as data:\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "aecc08d1-4886-41b6-9525-6e5055784a5b"
    }
   },
   "outputs": [],
   "source": [
    "best_alpha = 32\n",
    "model = Ridge(alpha=best_alpha)\n",
    "model.fit(x_train, y_train)\n",
    "y_test_predict = model.predict(x_test)\n",
    "\n",
    "print(\"Test error\")\n",
    "print(mean_squared_error(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "866cf2e4-6d82-494b-b4b8-0856defe29d5"
    }
   },
   "source": [
    "## Summary:\n",
    "* We're primarily interested in predictive performance on unseen data, not on seen data.\n",
    "* **Training error** estimates error on **seen** data\n",
    "* **Cross validation error** estimates error on **unseen** data\n",
    "* **Regularization** adds an additional term to our **loss function** that penalized parameter squared-magnitude $\\|\\boldsymbol{\\beta}\\|^{2}$. The **hyperparameter** $\\alpha$ tunes the relative contribution of this penalty term to the residual sum of squares term $\\sum_{i}\\left(y_{i}-X_{i}^{T}\\beta\\right)^{2}$\n",
    "* We discussed one type of regularization, **Ridge Regression**, but there are many other possibilities with their own advantages\n",
    "  * **Ridge Regression**: $\\alpha\\sum_{i}\\|\\beta_{i}\\|^{2}$    ('L2 penalty')\n",
    "  * **LASSO**: $\\alpha\\sum_{i}\\|\\beta_{i}\\|$    ('L1 penalty')\n",
    "  * **ElasticNet**: $\\alpha_{\\text{RR}}\\sum_{i}\\|\\beta_{i}\\|^{2}+\\alpha_{\\text{LASSO}}\\sum_{i}\\|\\beta_{i}\\|$    (Combination of Ridge and Lasso)\n",
    "* We talked about two kinds of cross validation error:\n",
    "  * **Validation Error** -- split your training set into a reduced training set and a validation set\n",
    "  * **K-Fold Error** -- Split your training data into k reduced training sets and a validation sets\n",
    "* Regularization introduces new hyperparameters\n",
    "* Use a cross validated estimate of future performance to choose your model and hyperparameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ce1bec1f-27f7-444f-9966-1fe808660967"
    }
   },
   "source": [
    "### Extra\n",
    "- In the remaining time, let's explore the differences between Ridge and LASSO regression\n",
    "- Notice the functional difference between the two regularization types (see above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e3e7bb91-c64f-4f64-ba4c-fcd7009fded0"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# load overfitting data into 'train' and 'test' sets\n",
    "with np.load('data/overfitting_data.npz') as data:\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Ridge and LASSO models with different alpha, and keep track of parameter magnitudes of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "de09e098-a8d9-4c4d-93d6-bd1d573db637"
    }
   },
   "outputs": [],
   "source": [
    "# Ridge\n",
    "alphas_ridge = [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 5096]\n",
    "coefs_ridge = np.zeros((len(alphas_ridge), 3))\n",
    "\n",
    "for i, alpha in enumerate(alphas_ridge):\n",
    "    ridge_model = Ridge(alpha = alpha)\n",
    "    \n",
    "    \n",
    "    ridge_model.fit(x_train, y_train)\n",
    "                    \n",
    "    coefs_ridge[i,:] = ridge_model.coef_[:3]\n",
    "    \n",
    "    \n",
    "# Lasso\n",
    "alphas_lasso = [.01, .03, .06, .1, .3, .6, 1, 3, 10]\n",
    "coefs_lasso = np.zeros((len(alphas_lasso), 3))\n",
    "\n",
    "for i, alpha in enumerate(alphas_lasso):\n",
    "    lasso_model = Lasso(alpha = alpha)\n",
    "    \n",
    "    lasso_model.fit(x_train, y_train)\n",
    "    \n",
    "    coefs_lasso[i,:] = lasso_model.coef_[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "73d68721-932a-474f-911e-3d9d075e5b74"
    }
   },
   "source": [
    "##### Plotting the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f723c811-bb68-4dcd-be8b-a8f322792bba"
    }
   },
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,6))\n",
    "\n",
    "# Ridge plot\n",
    "plt.sca(axes[0])\n",
    "for i, param in enumerate(coefs_ridge.T):\n",
    "    plt.semilogx(alphas_ridge, param, lw = 3)\n",
    "\n",
    "plt.title('Ridge regression', size = 24)\n",
    "plt.xlabel(r'$\\alpha$', size = 22)\n",
    "plt.ylabel(r'Coefficient value', size = 22)\n",
    "plt.grid()\n",
    "plt.tick_params(labelsize = 16)\n",
    "    \n",
    "# Lasso plot\n",
    "plt.sca(axes[1])\n",
    "for i, param in enumerate(coefs_lasso.T):\n",
    "    plt.semilogx(alphas_lasso, param, lw = 3, label = r'Feature ' + str(i))\n",
    "    \n",
    "plt.title('LASSO regression', size = 24)\n",
    "plt.xlabel(r'$\\alpha$', size = 22)\n",
    "plt.ylabel(r'Coefficient value', size = 22)\n",
    "plt.grid()\n",
    "plt.tick_params(labelsize = 16)\n",
    "plt.legend(loc = 'upper right', fontsize = 16)\n",
    "    \n",
    "# Save, display plot\n",
    "fig.tight_layout()\n",
    "#plt.savefig('../figures/session_2/ridge_LASSO.png')\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
