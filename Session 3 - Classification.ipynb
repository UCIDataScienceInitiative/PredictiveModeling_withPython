{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://datascience.uci.edu/wp-content/uploads/sites/2/2014/09/data_science_logo_with_image1.png 'UCI_data_science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling with Python \n",
    "## Session #3: _Linear Methods for Classification_\n",
    "**Author:** [Eric Nalisnick](http://www.ics.uci.edu/~enalisni/)\n",
    "\n",
    "**Modified by:** Preston Hinkle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of this Lesson\n",
    "- Extend the regression framework to support classification\n",
    "    - Logistic Regression\n",
    "    - Training with Gradient Descent\n",
    "    - Training with Newton's Method\n",
    "    \n",
    "- Implement... \n",
    "    - The Logistic function\n",
    "    - A function to compute the Hessian matrix\n",
    "    - An instantiation of SciKit-Learn's Logistic regression class\n",
    "    - One-vs-rest (OVR) logistic regression for more than 2 classes\n",
    "\n",
    "### References \n",
    "- Chapter 4 of [*Elements of Statistical Learning* by Hastie, Tibshirani, Friedman](http://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf) \n",
    "- [A Few Useful Things to Know about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "- [SciKit-Learn's Logistic Regression Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "\n",
    "## 0.  Python Preliminaries\n",
    "As usual, first we need to import Numpy, Pandas, MatPlotLib..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created two functions that we'll use later to visualize which datapoints are being assigned to which classes.  They are a bit messy and not essential to the material so don't worry about understanding them.  I'll be happy to explain them to anyone interested during a break or after the session.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# A somewhat complicated function to make pretty plots\n",
    "def plot_classification_data(data1, data2, beta, logistic_flag=False):\n",
    "    plt.figure()\n",
    "    grid_size = .2\n",
    "    features = np.vstack((data1, data2))\n",
    "    # generate a grid over the plot\n",
    "    x_min, x_max = features[:, 0].min() - .5, features[:, 0].max() + .5\n",
    "    y_min, y_max = features[:, 1].min() - .5, features[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, grid_size), np.arange(y_min, y_max, grid_size))\n",
    "    # color the grid based on the predictions \n",
    "    if logistic_flag:\n",
    "        Z = logistic(np.dot(np.c_[xx.ravel(), yy.ravel(), np.ones(xx.ravel().shape[0])], beta))\n",
    "        colorbar_label=r\"Value of f($X \\beta)$\"\n",
    "    else:\n",
    "        Z = np.dot(np.c_[xx.ravel(), yy.ravel(), np.ones(xx.ravel().shape[0])], beta)\n",
    "        colorbar_label=r\"Value of $X \\beta$\"\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    background_img = plt.pcolormesh(xx, yy, Z, cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    # Also plot the training points\n",
    "    plt.scatter(class1_features[:, 0], class1_features[:, 1], c='b', edgecolors='k', s=70, alpha = 0.75, marker = 'x')\n",
    "    plt.scatter(class2_features[:, 0], class2_features[:, 1], c='r', edgecolors='k', s=70, alpha = 0.75, marker = '+')\n",
    "    plt.title('Data with Class Prediction Intensities')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    color_bar = plt.colorbar(background_img, orientation='horizontal')\n",
    "    color_bar.set_label(colorbar_label)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()\n",
    "\n",
    "# Another messy looking function to make pretty plots of basketball courts\n",
    "def visualize_court(log_reg_model, court_image = './data/nba_experiment/nba_court.jpg'):\n",
    "    two_class_cmap = ListedColormap(['#FFAAAA', '#AAFFAA']) # light red for miss, light green for make\n",
    "    x_min, x_max = 0, 50 #width (feet) of NBA court\n",
    "    y_min, y_max = 0, 47 #length (feet) of NBA half-court\n",
    "    grid_step_size = 0.2\n",
    "    grid_x, grid_y = np.meshgrid(np.arange(x_min, x_max, grid_step_size), np.arange(y_min, y_max, grid_step_size))\n",
    "    grid_predictions = log_reg_model.predict(np.c_[grid_x.ravel(), grid_y.ravel()])\n",
    "    grid_predictions = grid_predictions.reshape(grid_x.shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    court_image = plt.imread(court_image)\n",
    "    ax.imshow(court_image, interpolation='bilinear', origin='lower',extent=[x_min,x_max,y_min,y_max])\n",
    "    ax.imshow(grid_predictions, cmap=two_class_cmap, interpolation = 'nearest',\n",
    "              alpha = 0.60, origin='lower',extent=[x_min,x_max,y_min,y_max])\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.title( \"Make / Miss Prediction Boundaries\" )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Classes as Targets\n",
    "\n",
    "Now that we've seen how to train and evaluate a linear model for real-valued responses, next we turn to classification.  At first glance, jumping from regression to classification seems trivial.  Say there are two classes, the first denoted by 0 and the second by 1.  We could just set each $y_{i}$ to 0 or 1 according to its class membership and fit a linear model just as before.\n",
    "\n",
    "_** Here's an example doing just that on some artificial data... **_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### function for shuffling the data and labels\n",
    "def shuffle_in_unison(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "    \n",
    "### calculate classification errors\n",
    "# return a percentage: (number misclassified)/(total number of datapoints)\n",
    "def calc_classification_error(predictions, class_labels):\n",
    "    n = predictions.size\n",
    "    num_of_errors = 0.\n",
    "    for idx in xrange(n):\n",
    "        if (predictions[idx] >= 0.5 and class_labels[idx]==0) or (predictions[idx] < 0.5 and class_labels[idx]==1):\n",
    "            num_of_errors += 1\n",
    "    return num_of_errors/n\n",
    "\n",
    "### false positive rate\n",
    "# returns fraction of one predictions that are incorrect\n",
    "def get_false_positive_rate(predictions, class_labels):\n",
    "    binary_predictions = copy.copy(predictions)\n",
    "    binary_predictions[binary_predictions >= 0.5] = 1\n",
    "    binary_predictions[binary_predictions < 0.5] = 0\n",
    "    \n",
    "    total_count = predictions.shape[0]\n",
    "    \n",
    "    false_positive_count = binary_predictions[(class_labels == 1) & (binary_predictions == 0)].shape[0]\n",
    "    \n",
    "    return 1.*false_positive_count/total_count\n",
    "    \n",
    "### false negative rate\n",
    "# returns fraction of zero predictions that are incorrect\n",
    "def get_false_negative_rate(predictions, class_labels):\n",
    "    binary_predictions = copy.copy(predictions)\n",
    "    binary_predictions[binary_predictions >= 0.5] = 1\n",
    "    binary_predictions[binary_predictions < 0.5] = 0\n",
    "    \n",
    "    total_count = predictions.shape[0]\n",
    "    \n",
    "    false_negative_count = binary_predictions[(class_labels == 0) & (binary_predictions == 1)].shape[0]\n",
    "    \n",
    "    return 1.*false_negative_count/total_count\n",
    "\n",
    "# set the random number generator for reproducability\n",
    "np.random.seed(182)\n",
    "\n",
    "#### create artificial data\n",
    "N = 400\n",
    "D = 2\n",
    "# Sample the features from a Multivariate Normal Dist.\n",
    "mean1 = [13,5]\n",
    "mean2 = [5,5]\n",
    "covariance = [[13,5],[5,13]]\n",
    "class1_features = np.random.multivariate_normal(mean1,covariance,N/2)\n",
    "class2_features = np.random.multivariate_normal(mean2,covariance,N/2)\n",
    "features = np.vstack((class1_features, class2_features))\n",
    "# add column of ones for bias term\n",
    "features = np.hstack((features,np.ones((N,1))))\n",
    "# Set the class labels\n",
    "class1_labels = [0]*(N/2)\n",
    "class2_labels = [1]*(N/2)\n",
    "class_labels = class1_labels+class2_labels\n",
    "# shuffle the data\n",
    "shuffle_in_unison(features, class_labels)\n",
    "class_labels = np.array(class_labels)[np.newaxis].T\n",
    "\n",
    "### fit the linear model --- OLS Solution\n",
    "beta = np.dot(np.linalg.inv(np.dot(features.T, features)),np.dot(features.T,class_labels))\n",
    "\n",
    "### compute error on training data\n",
    "predictions = np.dot(features, beta)\n",
    "print \"Classification Error on Training Set: %.2f%%\" %(calc_classification_error(predictions, class_labels) * 100)\n",
    "\n",
    "# False positives and negatives\n",
    "print 'False positive rate = ', get_false_positive_rate(predictions, class_labels)*100\n",
    "print 'False negative rate = ', get_false_negative_rate(predictions, class_labels)*100\n",
    "\n",
    "### generate a plot\n",
    "plot_classification_data(class1_features, class2_features, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked okay.  9.75% error is respectable.  Yet, let's think a bit harder about what's going on...\n",
    "* It seems a bit arbitary to set the class labels to 0 vs. 1.  Why couldn't we have set them to -1 vs. +1?  Or 500 vs. 230?  The responses don't have the same intrinsic meaning they did before.  Now the labels represent exclusive class membership whereas before they represented physical quantities (baseball player's salary, for example).      \n",
    "* During training, we're optimizing squared error, but the metric we truly care about is classification percentage.  Squared distance seems inappropriate especially when it's not even clear to what value the responses should be set.\n",
    "\n",
    "## 1. Modifying the loss function\n",
    "\n",
    "Here's an idea: since we care primarily about classification error, let's make that our loss function...\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mathcal{L}_{\\mathrm{class}} = \\begin{cases} 1, & \\text{if $y_{i}\\ne$round($\\hat y_{i}$).}\\\\0, & \\text{otherwise}.\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where $\\hat y_{i}$ is our model's prediction of label $y_{i} \\in \\{0,1\\}$ and round() sends $\\hat y_{i}$ to 0 or 1, whichever is closer. Great.  Now all we have to do is perform gradient descent to train the model...wait a minute...$\\mathcal{L}_{\\mathrm{class}}$ isn't differentiable.  \n",
    "\n",
    "Let's consider another loss function:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mathcal{L} = \\sum_{i=1}^{N} -y_{i} \\log \\hat y_{i} - (1-y_{i}) \\log (1-\\hat y_{i})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where, again, $\\hat y_{i}$ is our model's prediction of label $y_{i} \\in \\{0,1\\}$.  Here $\\log$ will refer to the natural logarithm, base $e$.  This is called the *cross-entropy* error function.  Notice it's well-suited for classification in that it is directly optimizing towards $0$ and $1$.  To see this, let $y_{i}=1$.  In that case, the second term is zero (due to the $1-y_{i}$ coefficient) and the loss becomes $\\mathcal{L}= - \\log \\hat y_{i}$.  Recall that $-\\log \\hat y_{i} = 0$ when $\\hat y_{i}=1$ and that $-\\log \\hat y_{i} = \\infty$ when $\\hat y_{i}=0$.  Thus, we are encouraging $\\hat y_{i}$ to become equal to $1$, its class label, and incurring penalty the more it moves towards $0$.\n",
    "\n",
    "Advanced Note: Cross-entropy loss still may seem arbitrary to some readers.  It is derived by taking the negative logarithm of the Bernoulli distribution's density function, which has support {0,1}.  Therefore, we can think of each class label as the result of a Bernoulli trial--a parameterized coin flip, essentially.  Many loss functions are merely the negative logarithm of some probability density function.  Squared error is derived by taking the $-\\log$ of the Normal density funciton.\n",
    "\n",
    "\n",
    "\n",
    "## 2.  Modifying the Linear Model\n",
    "\n",
    "Now that we have our loss function and proper labels, we turn to the model itself, represented by the parameter $\\hat y$ above.  What if we keep define $\\hat y$ just as we did for linear regression?\n",
    "\\begin{equation*}\n",
    "\\hat y_{i} = \\beta_0 + \\beta_1 x_{i,1} + \\dots + \\beta_p x_{i,D} = \\mathbf{x}_i^T \\mathbf{\\beta}\n",
    "\\end{equation*}\n",
    "Notice parameterizing $\\hat y_{i}$ with $\\mathbf{x}_i^T\\beta$ doesn't work since the value would be unconstrained and result in the loss being undefined if $\\hat y\\le 0$.  Thus, we need a function $f$ such that $f:\\mathbb{R} \\mapsto (0,1)$.  We can probably think-up many functions that have a range on this interval so we'll limit the functions we can use by specifying two more requirements: the function must *(1)* be differentiable (in order to perform gradient descent) and *(2)* have a probabilistic interpretation (to think of the output as the probability the input is in class 1).  \n",
    "\n",
    "Cumulative Distribution Functions (CDFs) have all of these nice properties.  They 'squeeze' their input onto $(0,1)$, are differentiable (since that's how a pdf is derived) and have a probabilistic interpretation.  In this case, we can use any CDF as long as it has support on $(-\\infty, +\\infty)$ since this is the range of $X_i^T\\beta$.\n",
    "\n",
    "Choosing which CDF to use can be a hard decision since each choice drags along assumptions we don't have time to go into here.  We'll choose the Logistic Distribution's CDF:\n",
    "\\begin{equation*}\n",
    "f(z; 0, 1) = \\frac{1}{1+e^{-z}}.\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![alt text](http://deeplearning.net/software/theano/_images/logistic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tradition partly dictates this choice, but it does provide the nice interpretation that $x_i^T\\beta$ is modeling the 'log odds':\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log \\frac{\\hat y}{1-\\hat y} &=& \\log \\frac{f(z; 0, 1)}{1-f(z; 0, 1)} \\\\ &=& \\log f(z; 0, 1) - \\log (1-f(z; 0, 1) )\\\\ &=& -\\log (1+e^{-z}) - \\log (1-(1+e^{-z})^{-1}) \\\\ &=& -\\log (1+e^{-z}) - \\log e^{-z} + \\log (1+e^{-z}) \\\\ &=&  - \\log e^{-z} \\\\ &=& z   \\\\ &=& \\mathbf{x}_i^T \\mathbf{\\beta}  \\end{eqnarray*} \n",
    "\n",
    "This use of the Logistic Distribution is where Logistic Regression gets its name.  As a side note before proceeding, using the Normal CDF instead of the Logistic is called 'Probit Regression,' the second most popular regression framework.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">STUDENT ACTIVITY (5 MINS)</span> \n",
    "The Logistic transformation function is the key to extending regression to classification.  Below you'll see the function *def logistic(z)*.  Complete it by filling in the logistic function and then graph the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the transformation function\n",
    "def logistic(z):\n",
    "    # TO DO: return the output of the logistic function\n",
    "\n",
    "# a few tests to make sure your function is working\n",
    "print \"Should print 0.5:\"\n",
    "print logistic(0)\n",
    "print\n",
    "print \"Should print 0.81757...:\"\n",
    "print logistic(1.5)\n",
    "print\n",
    "# needs to handle arrays too\n",
    "print \"Should print [ 0.450166    0.5124974   0.98201379]:\"\n",
    "print logistic(np.array([-.2,.05,4]))\n",
    "print\n",
    "\n",
    "# graph the function \n",
    "z = np.linspace(-6,6,50)\n",
    "logistic_out = logistic(z)\n",
    "plt.figure()\n",
    "\n",
    "# TO DO: write the line of code to plot the function\n",
    "plt.plot(z, logistic(z))\n",
    "\n",
    "plt.title(\"Logistic Function\")\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Logistic Regression: A Summary\n",
    "\n",
    "_**Data**_\n",
    "\n",
    "We observe pairs $(\\mathbf{x}_{i},y_{i})$ where\n",
    "\\begin{eqnarray*}\n",
    "y_{i} \\in \\{ 0, 1\\} &:& \\mbox{class label} \\\\\n",
    "\\mathbf{x}_{i} = (1, x_{i,1}, \\dots, x_{i,D}) &:& \\mbox{set of $D$ explanatory variables (aka features) and a bias term } \n",
    "\\end{eqnarray*}\n",
    "\n",
    "_** Parameters**_\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{\\beta}^{T} = (\\beta_{0}, \\dots, \\beta_{D}) : \\mbox{values encoding the relationship between the features and label}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "_** Transformation Function **_\n",
    "\n",
    "\\begin{equation*}\n",
    "f(z_{i}=\\mathbf{x}_{i} \\mathbf{\\beta} ) = (1+e^{-\\mathbf{x}_{i} \\mathbf{\\beta} })^{-1}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Error Function**_\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mathcal{L} = \\sum_{i=1}^{N} -y_{i} \\log f(\\mathbf{x}_{i} \\mathbf{\\beta} ) - (1-y_{i}) \\log (1-f(\\mathbf{x}_{i} \\mathbf{\\beta} ))\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### compute the cross-entropy error \n",
    "# labels: Numpy array containing the true class labels\n",
    "# f: column vector of predictions (i.e. output of logistic function)\n",
    "def cross_entropy(labels, f):\n",
    "    return np.sum(-1*np.multiply(labels,np.log(f)) - np.multiply((np.ones(N)-labels),np.log(np.ones(N)-f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_** Learning $\\beta$ **_\n",
    "\n",
    "Like Linear Regression, learning a Logistic Regression model will entail minimizing the error function $\\mathcal{L}$ above.  Can we solve for $\\beta$ in closed form?  Let's look at the derivative of $\\mathcal{L}$ with respect to $\\beta$:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial \\mathcal{L}_{i}}{\\partial \\mathbf{\\beta}} &=& \\frac{\\partial \\mathcal{L}_{i}}{\\partial f(z_{i})} \\frac{\\partial f(z_{i})}{\\partial z_{i}} \\frac{\\partial z_{i}}{\\partial \\mathbf{\\beta}}\\\\\n",
    "&=& \\left[\\frac{-y_{i}}{f(\\mathbf{x}_{i} \\mathbf{\\beta})} - \\frac{y_{i}-1}{1-f(\\mathbf{x}_{i} \\mathbf{\\beta})} \\right] f(\\mathbf{x}_{i} \\mathbf{\\beta})(1-f(\\mathbf{x}_{i} \\mathbf{\\beta}))\\mathbf{x}_{i}\\\\\n",
    "&=& [-y_{i}(1-f(\\mathbf{x}_{i} \\mathbf{\\beta} )) - (y_{i}-1)f(\\mathbf{x}_{i} \\mathbf{\\beta} )]\\mathbf{x}_{i}\\\\\n",
    "&=& [f(\\mathbf{x}_{i} \\mathbf{\\beta} ) - y_{i}]\\mathbf{x}_{i}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### compute the gradient (derivative w.r.t. Beta)\n",
    "# features: NxD feature matrix\n",
    "# labels: Numpy array containing the true class labels\n",
    "# f: column vector of predictions (i.e. output of logistic function)\n",
    "def compute_Gradient(features, labels, f):\n",
    "    return np.sum(np.multiply(f-labels,features),0)[np.newaxis].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first derivative contains the term $f(X_{i}\\beta)$, meaning the gradient depends on $\\beta$ in some non-linear way.  We have no choice but to use the Gradient Descent algorithm:\n",
    "- Randomly initialize $\\beta$\n",
    "- Until $\\alpha || \\nabla \\mathcal{L} || < tol $:\n",
    "    - $\\mathbf{\\beta}_{t+1} = \\mathbf{\\beta}_{t} - \\alpha \\nabla_{\\mathbf{\\beta}} \\mathcal{L}$\n",
    "\n",
    "_** Putting it all together in a simple example... **_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the random number generator for reproducability\n",
    "np.random.seed(49)\n",
    "\n",
    "# Randomly initialize the Beta vector\n",
    "beta = np.random.multivariate_normal([0,0,0], [[1,0,0],[0,1,0],[0,0,1]], 1).T\n",
    "\n",
    "\n",
    "# Initialize the step-size\n",
    "alpha = 0.00001\n",
    "# Initialize the gradient\n",
    "grad = np.infty\n",
    "# Set the tolerance \n",
    "tol = 1e-8\n",
    "# Initialize error\n",
    "old_error = 0\n",
    "error = [np.infty]\n",
    "\n",
    "# Run Gradient Descent\n",
    "start_time = time.time()\n",
    "iter_idx = 1\n",
    "# loop until gradient updates become small\n",
    "while (alpha*np.linalg.norm(grad) > tol) and (iter_idx < 300):\n",
    "    f = logistic(np.dot(features,beta))\n",
    "    old_error = error[-1]\n",
    "    # track the error\n",
    "    error.append(cross_entropy(class_labels, f)) \n",
    "    grad = compute_Gradient(features, class_labels, f)\n",
    "    # update parameters\n",
    "    beta = beta - alpha*grad\n",
    "    iter_idx += 1\n",
    "end_time = time.time()\n",
    "print \"Training ended after %i iterations, taking a total of %.2f seconds.\" %(iter_idx, end_time-start_time)\n",
    "print \"Final Cross-Entropy Error: %.2f\" %(error[-1])\n",
    "\n",
    "# compute error on training data\n",
    "predictions = logistic(np.dot(features, beta))\n",
    "print \"Classification Error on Training Set: %.2f%%\" %(calc_classification_error(predictions, class_labels) * 100)\n",
    "\n",
    "# False positives and negatives\n",
    "print 'False positive rate = ', get_false_positive_rate(predictions, class_labels)*100\n",
    "print 'False negative rate = ', get_false_negative_rate(predictions, class_labels)*100\n",
    "\n",
    "# generate the plot\n",
    "plot_classification_data(class1_features, class2_features, beta, logistic_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Newton's Method\n",
    "\n",
    "Choosing the step-size, $\\alpha$, can be painful since there is no principled way to set it.  We have little intuition for what parameter space really looks like and therefore no sense of how to move most efficiently.  Knowing the curvature of the space will solve this problem (to some extent).  Therefore, we arrive at Newton's Method:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\beta_{t+1} = \\beta_{t} - (\\frac{\\partial^{2} \\mathcal{L}}{\\partial \\beta \\partial \\beta^{T}})^{-1} \\nabla_{\\beta} \\mathcal{L}\n",
    "\\end{equation*}\n",
    "\n",
    "where $(\\frac{\\partial^{2} \\mathcal{L}}{\\partial \\beta \\partial \\beta^{T}})^{-1}$ is the inverse of the matrix of second derivatives, also known as the Hessian Matrix.  For Logistic regression, the Hessian is\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial^{2} \\mathcal{L}}{\\partial \\beta \\partial \\beta^{T}} = \\mathbf{X}^{T}\\mathbf{A}\\mathbf{X}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mathbf{A}= \\mathrm{diag}(f(X_{i}\\beta)(1-f(X_{i}\\beta)))$, a matrix with $f''$ along its diagonal.\n",
    "\n",
    "Our new parameter update is:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\beta_{t+1} &=& \\beta_{t} - (\\mathbf{X}^{T}\\mathbf{A}\\mathbf{X})^{-1}\\mathbf{X}^{T}[f(\\mathbf{X}\\beta) - \\mathbf{y}]\n",
    "\\end{eqnarray*}\n",
    "\n",
    "As you can see, we no longer need to specify a step-size.  We've replaced $\\alpha$ with $(\\frac{\\partial^{2} \\mathcal{L}}{\\partial \\beta \\partial \\beta^{T}})^{-1}$ and everything else stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">STUDENT ACTIVITY (10 MINS)</span> \n",
    "\n",
    "Write a function that computes the Hessian matrix ($\\mathbf{X}^{T}\\mathbf{A}\\mathbf{X}$).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_Hessian(features, f):\n",
    "    # X = feature matrix, size NxD), \n",
    "    # f = predictions (logistic outputs), size Nx1\n",
    "    # TO DO: return the Hessian matrix, size DxD \n",
    "\n",
    "\n",
    "# a few tests to make sure your function is working\n",
    "X = np.array([[1,2],[3,4],[5,6]])\n",
    "f = np.array([.1,.3,.5])[np.newaxis].T\n",
    "print \"Should print [[  8.23  10.2 ];[ 10.2   12.72]]:\"\n",
    "print compute_Hessian(X,f)\n",
    "print\n",
    "X = np.array([[1],[4],[6]])\n",
    "f = np.array([.01,.13,.55])[np.newaxis].T\n",
    "print \"Should print [[ 10.7295]]:\"\n",
    "print compute_Hessian(X,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_** Let's try Newton's Method on our simple example... **_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the random number generator for reproducability\n",
    "np.random.seed(1801843607)\n",
    "\n",
    "# Save the errors from run above\n",
    "no_Newton_errors = error\n",
    "\n",
    "# Randomly initialize the Beta vector\n",
    "beta = np.random.multivariate_normal([0,0,0], [[.1,0,0],[0,.1,0],[0,0,.1]], 1).T\n",
    "# Initialize error\n",
    "old_error = 0\n",
    "error = [np.infty]\n",
    "\n",
    "# Run Newton's Method\n",
    "start_time = time.time()\n",
    "iter_idx = 1\n",
    "# Loop until error doesn't change (as opposed to gradient)\n",
    "while (abs(error[-1] - old_error) > tol) and (iter_idx < 300):\n",
    "    f = logistic(np.dot(features,beta))\n",
    "    old_error = error[-1]\n",
    "    # track the error\n",
    "    error.append(cross_entropy(class_labels, f)) \n",
    "    grad = compute_Gradient(features, class_labels, f)\n",
    "    hessian = compute_Hessian(features,f)\n",
    "    # update parameters via Newton's method\n",
    "    beta = beta - np.dot(np.linalg.inv(hessian),grad)\n",
    "    iter_idx += 1\n",
    "end_time = time.time()\n",
    "print \"Training ended after %i iterations, taking a total of %.2f seconds.\" %(iter_idx, end_time-start_time)\n",
    "print \"Final Cross-Entropy Error: %.2f\" %(error[-1])\n",
    "\n",
    "# compute the classification error on training data\n",
    "predictions = logistic(np.dot(features, beta))\n",
    "print \"Classification Error on Training Set: %.2f%%\" %(calc_classification_error(predictions, class_labels) * 100)\n",
    "\n",
    "# False positives and negatives\n",
    "print 'False positive rate = ', get_false_positive_rate(predictions, class_labels)*100\n",
    "print 'False negative rate = ', get_false_negative_rate(predictions, class_labels)*100\n",
    "\n",
    "# generate the plot\n",
    "plot_classification_data(class1_features, class2_features, beta, logistic_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training progress to see how much more efficient Newton's method is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot difference between with vs without Newton\n",
    "plt.figure()\n",
    "# grad descent w/ step size\n",
    "plt.plot(range(len(no_Newton_errors)), no_Newton_errors, 'k-', linewidth=4, label='Without Newton')\n",
    "# newton's method\n",
    "plt.plot(range(len(error)), error, 'g-', linewidth=4, label='With Newton')\n",
    "plt.ylim([0,300000])\n",
    "plt.xlim([0,150])\n",
    "plt.legend()\n",
    "plt.title(\"Newton's Method vs. Gradient Descent w/ Step Size\")\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Cross-Entropy Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Logistic Regression with SciKit-Learn \n",
    "\n",
    "[Here is the documentation for SciKit-Learn's implementation of Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "It's quite easy to use.  Let's jump right in and repeat the above experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# set the random number generator for reproducability\n",
    "np.random.seed(75)\n",
    "\n",
    "#Initialize the model\n",
    "skl_LogReg = LogisticRegression(fit_intercept = False, C = 1000.)  \n",
    "\n",
    "#Train it\n",
    "start_time = time.time()\n",
    "\n",
    "skl_LogReg.fit(features, np.ravel(class_labels))\n",
    "end_time = time.time()\n",
    "print \"Training ended after %.2f seconds.\" %(end_time-start_time)\n",
    "\n",
    "# compute the classification error on training data\n",
    "predictions = skl_LogReg.predict(features).reshape(-1,1)\n",
    "print \"Classification Error on Training Set: %.2f%%\" %(calc_classification_error(predictions, class_labels) * 100)\n",
    "\n",
    "# False positives and negatives\n",
    "print 'False positive rate = ', get_false_positive_rate(predictions, class_labels)*100\n",
    "print 'False negative rate = ', get_false_negative_rate(predictions, class_labels)*100\n",
    "\n",
    "# generate the plot\n",
    "plot_classification_data(class1_features, class2_features, skl_LogReg.coef_.T, logistic_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "## 6.  Dataset #1: NBA Shot Outcomes\n",
    "\n",
    "The first real dataset we'll tackle is one describing the location and outcome of shots taken in professional basketball games.  Let's use Pandas to load and examine the data.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nba_shot_data = pd.read_csv('./data/nba_experiment/NBA_xy_features.csv')\n",
    "nba_shot_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nba_shot_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple enough.  Now let's train a Logistic Regression model on it, leaving out a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "train_set_size = int(.80*len(nba_shot_data))\n",
    "train_features = nba_shot_data.ix[:train_set_size,['x_Coordinate','y_Coordinate']]\n",
    "test_features = nba_shot_data.ix[train_set_size:,['x_Coordinate','y_Coordinate']]\n",
    "train_class_labels = nba_shot_data.ix[:train_set_size,['shot_outcome']]\n",
    "test_class_labels = nba_shot_data.ix[train_set_size:,['shot_outcome']]\n",
    "\n",
    "#Train it\n",
    "start_time = time.time()\n",
    "skl_LogReg = LogisticRegression(fit_intercept = True)\n",
    "skl_LogReg.fit(train_features, np.ravel(train_class_labels))\n",
    "end_time = time.time()\n",
    "print \"Training ended after %.2f seconds.\" %(end_time-start_time)\n",
    "\n",
    "# compute the classification error on training data\n",
    "predictions = skl_LogReg.predict(test_features)\n",
    "print \"Classification Error on the Test Set: %.2f%%\" %(calc_classification_error(predictions, np.array(test_class_labels)) * 100)\n",
    "\n",
    "# compute the baseline error since the classes are imbalanced\n",
    "print \"Baseline Error: %.2f%%\" %(np.sum(test_class_labels)/len(test_class_labels)*100)\n",
    "\n",
    "# visualize the boundary on the basketball court\n",
    "visualize_court(skl_LogReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad.  We're beating the random baseline of 45% error.  However, visualizing the decision boundary exposes a systemic problem with using a linear model on this dataset: it is not powerful enough to adapt to the geometry of the court.  This is a domain-specific contraint that should be considered when selecting the model and features.  For instance, a Gaussian-based classifier works a bit better, achieving 39.02% error.  Its decision boundary is visualized below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/enalisnick/NBA_shot_analysis/master/results/spatial_features_results/Gaussian_Mixture_Model.png\" alt=\"\" style=\"width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do better by adding more features?  For instance, if we knew the position (Guard vs. Forward vs. Center) of the player taking the shot, would that help?  Let's try.  First, load a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first we need to extract the file from the zip\n",
    "import zipfile\n",
    "zip = zipfile.ZipFile('./data/nba_experiment/NBA_all_features.csv.zip')\n",
    "zip.extractall('./data/nba_experiment/')\n",
    "\n",
    "nba_all_features = pd.read_csv('./data/nba_experiment/NBA_all_features.csv')\n",
    "nba_all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to notice is that this data is noisy.  Look at row 2 above; it says a player made a dunk from 33 feet above the baseline--that's beyond the three point line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">STUDENT ACTIVITY (20 MINS)</span> \n",
    "\n",
    "Your task is to train Scikit-Learn's Logistic Regression model on the new NBA data.  The data is split into train and test features already.  Your task is to train SciKit-Learn's Logistic Regression model on the *train_features* and *train_class_labels* and then compute the test classification error--which should be around 38%-39%.  **BONUS:** If you sucessfully train the SciKit-Learn model, implement gradient descent or Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "train_features = nba_all_features.ix[:train_set_size,:'Center']\n",
    "test_features = nba_all_features.ix[train_set_size:,:'Center']\n",
    "train_class_labels = nba_all_features.ix[:train_set_size,['shot_outcome']]\n",
    "test_class_labels = nba_all_features.ix[train_set_size:,['shot_outcome']]\n",
    "\n",
    "########## TO DO: TRAIN SCIKIT-LEARN'S LOG. REG. MODEL ##########\n",
    "\n",
    "# Your code goes here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(train_features, train_class_labels)\n",
    "ys = model.predict(test_features)\n",
    "print calc_classification_error(ys, np.array(test_class_labels))*100.\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "# compute the baseline error since the classes are imbalanced\n",
    "print \"Baseline Error: %.2f%%\" %(np.sum(test_class_labels)/len(test_class_labels)*100)\n",
    "\n",
    "# we can't visualize since D>2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  We've improved by a few percentage points.  Let's look at which features the model weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(nba_all_features):\n",
    "    if idx<11:\n",
    "        print \"%s: %.2f\" %(feature, model.coef_[0][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the classifier exploited the location features very little.  The position of the player was much more important, especially if he was a center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7.  Dataset #2: 20 News Groups\n",
    "\n",
    "For the second experiment, we'll work with the very popular '20 News Groups' dataset consisting of, well, 20 different categories of articles.  SciKit-Learn already has it ready for import.\n",
    "\n",
    "Let's use sklearn to build a model that can automatically classify articles based on their text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# use SciKit Learn's loading methods\n",
    "categories = ['soc.religion.christian', 'alt.atheism']\n",
    "train_20ng = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=categories)\n",
    "test_20ng = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=categories)\n",
    "\n",
    "# transform the text into word counts\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "train_vectors = vectorizer.fit_transform(train_20ng.data)\n",
    "test_vectors = vectorizer.transform(test_20ng.data) #use the transform fit to the training data\n",
    "\n",
    "train_targets = train_20ng.target\n",
    "test_targets = test_20ng.target\n",
    "\n",
    "print \"The training data size is \"+str(train_vectors.shape)\n",
    "print \"The test data size is \"+str(test_vectors.shape)\n",
    "# print the first 500 words of an article\n",
    "print \"Example text:\"\n",
    "print train_20ng.data[0][:500]\n",
    "print\n",
    "print \"Example count vector:\"\n",
    "print train_vectors[0].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the vector is super sparse and very high dimensional--much different than the data we've been working with previously.  Let's see how SciKit-Learn's Logistic Regression model handles it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train it\n",
    "start_time = time.time()\n",
    "skl_LogReg.fit(train_vectors, train_targets)\n",
    "end_time = time.time()\n",
    "print \"Training ended after %.2f seconds.\" %(end_time-start_time)\n",
    "\n",
    "# compute the classification error on training data\n",
    "predictions = skl_LogReg.predict(test_vectors)\n",
    "print \"Classification Error on the Test Set: %.2f%%\" %(calc_classification_error(predictions, test_targets) * 100)\n",
    "\n",
    "# compute the baseline error since the classes are imbalanced\n",
    "print \"Baseline Error: %.2f%%\" %(100 - sum(test_targets)*100./len(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24% error is respectable, but there's still room for improvement.  In general, working with natural language is one of the hardest application domains in Machine Learning due to the fact that we often have to reduce the abstract, sometimes ambiguous semantic meaning to a superficial token.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going beyond binary classifier models\n",
    "\n",
    "So far we've only looked at **binary classification** problems where our model's output was limited to {0,1}. \n",
    "\n",
    "However, many useful classification problems have more than two categories to predict.\n",
    "\n",
    "There are a number of different ways to change logistic regression to allow for more than two outputs.\n",
    "\n",
    "One popular and intuitive method which sklearn uses by default is the 'One-vs-Rest' (OVR) scheme.\n",
    "\n",
    "In OVR, we train one binary model for every output class $k$ that predicts whether the data point belongs to class $k$ or does not belong to class $k$; each trained binary model makes probability predictions P('is class k' (1)) and P('is not class k' (0)).\n",
    "\n",
    "We choose the output category to be the one which had the greatest probability of all the OVR predicted probabilities.\n",
    "\n",
    "For instance, given a 3-class classification problem with the following probabilities, we would predict class 3 for the following data point:\n",
    "\n",
    "- Class k = 1: P(k=1) = 0.1, P(k!=1) = 0.9\n",
    "- Class k = 2: P(k=2) = 0.3, P(k!=2) = 0.7\n",
    "- **Class k = 3: P(k=3) = 0.8, P(k!=3) = 0.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dataset # 3: Hand-written digits\n",
    "\n",
    "Let's use sklearn.LogisticRegression to predict hand-written digits that have been digitized into an 8x8 matrix. Here's what one of those digits looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Don't change this!\n",
    "######################################################\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "digits = load_digits()\n",
    "plt.imshow(digits['images'][0], cmap = 'gray', interpolation='none')\n",
    "plt.show()\n",
    "print 'An example ' + str(digits['target'][0]) + ' digit.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">STUDENT ACTIVITY (20 minutes)</span> \n",
    "\n",
    "- Let's build a model to classify digits!\n",
    "- The training data is contained in the `dict` object `digits` defined above\n",
    "- We can use a one-dimensional array of data\n",
    "Instructions:\n",
    "    1. Split the digits data into a training and test set, with the test set composed of the last **200** digits in the set (there is no need to shuffle).\n",
    "        - Hints: \n",
    "            - The data is contained in a Python `dict`, which is accessed like a list but with keywords (keys) instead of numbered indices. For instance, to get the values of the pixels we use `digits['images']`, and to get the labels of the digits we use `digits['target']`.\n",
    "            - The features should be a 1-D list of pixel values so that the total training set X_train is a (num samples)x(num pixels) matrix, or numpy array. But, each example's features obtained from `digits['images'][index]` come by default as 8x8 arrays. The `np.reshape` function will be helpful to collapse the 8x8 array into a 64x1 array.\n",
    "    2. Train a LogisticRegression() model on the training data; remember, LogisticRegression() takes care of the OVR scheme for you!\n",
    "    3. Test the model on the testing data.\n",
    "    4. What was the accuracy of the model? Hint: Use the model.score(X_test, y_test) function to get the classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Insert solution below!\n",
    "######################################################\n",
    "\n",
    "n_pixels = 64\n",
    "\n",
    "# Set number of test and training examples\n",
    "n_test = 200\n",
    "n_train = digits['images'].shape[0] - n_test    # First element on RHS gives total number of examples in the data set;\n",
    "                                                # We subtract off n_test since those aren't in our training set\n",
    "\n",
    "\n",
    "# Get training features and training labels`\n",
    "X_train = digits['images'][:n_train]                                 # Load first n_train digits as the training data\n",
    "X_train = X_train.reshape(n_train, 64)    # Reshape the training array\n",
    "y_train = digits['target'][:n_train]\n",
    "\n",
    "# Get test features and test labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create, train a LogisticRegression model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the test data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- In this section we learned about **logistic regression**, a method for training a model to make **classification predictions**\n",
    "- Logistic regression can be seen as a modification of plain old linear regression with two major changes:\n",
    "    1. We 'squashed' the output between 0 and 1 using the **logistic function**; this has the nice interpretation that our output is the probability that that data point belongs to class 1.\n",
    "\\begin{equation*}\n",
    "f(z; 0, 1) = \\frac{1}{1+e^{-z}}.\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "z=\\mathbf{x}_{i}^T \\mathbf{\\beta}\n",
    "\\end{equation*}\n",
    "\n",
    "![alt text](http://deeplearning.net/software/theano/_images/logistic.png)\n",
    "\n",
    "    2. We created a new loss function---the cross-entropy error function---to go along with our new output\n",
    "\\begin{eqnarray*}\n",
    "\\mathcal{L} = \\sum_{i=1}^{N} -y_{i} \\log \\hat y_{i} - (1-y_{i}) \\log (1-\\hat y_{i})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- We used Newton's method to speed up the convergence time of gradient descent\n",
    "\n",
    "- We explored the 'One-vs-Rest' method for problems with more than 2 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional\n",
    "\n",
    "- In just one day, we managed to learn linear regression, logistic regression, and cross-validation and regularization.\n",
    "- Great job!\n",
    "- In the remaining time, you could practice the skills you've just learned on different data sets.\n",
    "- There's plenty of data out there on the internet to practice on, but a good starting point would be at our very own [UCI Machine Learning data repository!](http://archive.ics.uci.edu/ml/datasets.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
